{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2\n",
    "for each driver, creates a list of standard routes in that order so that the higher in the list a standard route is, the least the diversion of the driver will be, and \n",
    "the output of the program is: \n",
    "\n",
    "a file called driver.json that has for each driver, the 5 standard routes routes that if the driver does them, it minimizes the diversion. You can test this by considering as pool of standard routes those that originally the company has and also those that you recommend in the recStandard.json. The file driver.json has the following syntax:\n",
    "[\n",
    "\t{driver:C, routes:[s10, s20, s2, s6, s10}}, \n",
    "\t{driver:A, routes:[s1, s2, s22, s61, s102]}, \n",
    "â€¦.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME:  c:\\Users\\matti\\Desktop\\CODE\\DataMiningProject23-24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print('HOME: ',HOME)\n",
    "\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import lxml\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import HDBSCAN, DBSCAN\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy.sparse import csr_matrix, issparse, lil_matrix, coo_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from numba import njit, prange, jit\n",
    "from numba_progress import ProgressBar\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_FILE = 'standard_big_new_2.json'\n",
    "ACTUAL_FILE = 'actual_big_new_2.json'\n",
    "\n",
    "\n",
    "K_SHINGLES = 3\n",
    "ALPHA = 0.7 #TODO: not needed maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading standard data...\n",
      "\n",
      "Reading actual data...\n",
      "\n",
      "Creating standard dataframe...\n",
      "\n",
      "Creating actual dataframe...\n",
      "   id                                              route\n",
      "0  s0  [{'from': 'Caltanissetta', 'to': 'Piacenza', '...\n",
      "1  s1  [{'from': 'Rome', 'to': 'Cerignola', 'merchand...\n",
      "2  s2  [{'from': 'Massa', 'to': 'Treviso', 'merchandi...\n",
      "3  s3  [{'from': 'Cerignola', 'to': 'Perugia', 'merch...\n",
      "4  s4  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "   id driver sroute                                              route\n",
      "0  a0      D     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "1  a1      H     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "2  a2      I     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "3  a3      E     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "4  a4      J     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "\n",
      "Finished preparing standard data\n",
      "\n",
      "Finished preparing actual data\n",
      "\n",
      "Unique cities:  ['Caltanissetta', 'Cerignola', 'Foggia', 'Massa', 'Naples', 'Perugia', 'Piacenza', 'Rome', 'Treviso', 'Vicenza']\n",
      "Unique items:  ['Beach Towel', 'Canned Beans', 'Dumbbells', 'Lawn Mower', 'Lighter Fluid', 'Matches', 'Mayonnaise', 'Pain Relievers', 'Pens', 'Strawberries']\n",
      "Unique drivers:  ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
      "standardIds:  ['s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9']\n",
      "\n",
      "Number of cities:  10\n",
      "Number of items:  10\n",
      "\n",
      "Longest route:  17\n",
      "Shortest route:  1\n",
      "\n",
      "Max item quantity:  10\n",
      "\n",
      "Number of three-shingles:  720\n",
      "\n",
      "2-shingles:  90\n",
      "2-shingles:  45\n",
      "\n",
      "\u001b[92mK-Shingles used: 2 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load standard and actual data\n",
    "print(\"\\nReading standard data...\")\n",
    "with open(os.path.join('data',STANDARD_FILE)) as f:\n",
    "    standard = json.load(f)\n",
    "\n",
    "print(\"\\nReading actual data...\")\n",
    "with open(os.path.join('data', ACTUAL_FILE)) as f:\n",
    "    actual = json.load(f)\n",
    "\n",
    "# load the data into a dataframe\n",
    "print(\"\\nCreating standard dataframe...\")\n",
    "dfStandard = pd.DataFrame(standard)\n",
    "print(\"\\nCreating actual dataframe...\")\n",
    "dfActual = pd.DataFrame(actual)\n",
    "\n",
    "# print head of the dataframes\n",
    "print(dfStandard.head())\n",
    "print(dfActual.head())\n",
    "\n",
    "# get the unique cities and items of the standard data\n",
    "cities = []\n",
    "items = []\n",
    "drivers = []\n",
    "longestRoute = 0\n",
    "shortestRoute = np.inf\n",
    "maxItemQuantity = 0\n",
    "\n",
    "standardRefIds = []\n",
    "for index, s in dfStandard.iterrows():\n",
    "    #print(s)\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    standardRefIds.append(int(idS[1]))\n",
    "    for trip in route:\n",
    "        cities.append(trip['from']) \n",
    "        items.extend(trip['merchandise'].keys())\n",
    "        maxItemQuantity = max(maxItemQuantity, max(trip['merchandise'].values()))\n",
    "    if len(route) > 0:\n",
    "        cities.append(route[-1]['to'])\n",
    "        \n",
    "    if len(route) > longestRoute:\n",
    "        longestRoute = len(route)\n",
    "        \n",
    "    if len(route) < shortestRoute:\n",
    "        shortestRoute = len(route)\n",
    "print(\"\\nFinished preparing standard data\")\n",
    "\n",
    "actualRefStandardIds = []\n",
    "for index, s in dfActual.iterrows():\n",
    "    #print(s)\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    idStandard = s['sroute']\n",
    "    drivers.append(s['driver'])\n",
    "    actualRefStandardIds.append(int(idStandard[1]))\n",
    "    for trip in route:\n",
    "        cities.append(trip['from'])\n",
    "        items.extend(trip['merchandise'].keys())\n",
    "        maxItemQuantity = max(maxItemQuantity, max(trip['merchandise'].values()))\n",
    "        \n",
    "    if len(route) > 0:\n",
    "        cities.append(route[-1]['to'])\n",
    "        \n",
    "    if len(route) > longestRoute:\n",
    "        longestRoute = len(route)\n",
    "    \n",
    "    if len(route) < shortestRoute:\n",
    "        shortestRoute = len(route)\n",
    "print(\"\\nFinished preparing actual data\")\n",
    "\n",
    "# find the unique cities and items\n",
    "uniqueCities = sorted(list(set(cities)))\n",
    "#uniqueCities.insert(0, 'NULL')          # add NULL city, for padding vectors with different lengths (trips in routes)\n",
    "uniqueItems = sorted(list(set(items)))\n",
    "uniqueDrivers = sorted(list(set(drivers)))\n",
    "\n",
    "if shortestRoute < 2:\n",
    "    K_SHINGLES = 2\n",
    "\n",
    "threeShingles = []\n",
    "\n",
    "for i, c1 in enumerate(uniqueCities):\n",
    "    for j, c2 in enumerate(uniqueCities):\n",
    "        if i == j:\n",
    "            continue\n",
    "        for k, c3 in enumerate(uniqueCities):\n",
    "            if j == k or i == k:\n",
    "                continue\n",
    "            threeShingles.append([c1, c2, c3])\n",
    "            \n",
    "permutations = math.perm(len(uniqueCities), K_SHINGLES)\n",
    "\n",
    "print(\"\\nUnique cities: \", uniqueCities)\n",
    "print(\"Unique items: \", uniqueItems)\n",
    "print(\"Unique drivers: \", uniqueDrivers)\n",
    "\n",
    "standardIds = dfStandard['id'].tolist()\n",
    "print(\"standardIds: \", standardIds)\n",
    "\n",
    "print(\"\\nNumber of cities: \", len(uniqueCities))\n",
    "print(\"Number of items: \", len(uniqueItems))\n",
    "\n",
    "print(\"\\nLongest route: \", longestRoute)\n",
    "print(\"Shortest route: \", shortestRoute)\n",
    "\n",
    "print(\"\\nMax item quantity: \", maxItemQuantity)\n",
    "\n",
    "print(\"\\nNumber of three-shingles: \", len(threeShingles))\n",
    "\n",
    "print(f\"\\n{K_SHINGLES}-shingles: \", math.perm(len(uniqueCities), K_SHINGLES))\n",
    "print(f\"{K_SHINGLES}-shingles: \", math.comb(len(uniqueCities), K_SHINGLES))\n",
    "\n",
    "print(f\"\\n\\033[92mK-Shingles used: {K_SHINGLES} \\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashShingles(shingles, n):\n",
    "    # hash shingles\n",
    "    string = \"\" \n",
    "    for shingle in shingles:\n",
    "        string += str(shingle) + \",\" # [45, 4, 8] -> \"45,4,8,\"\n",
    "    \n",
    "    return hash(string) #% n\n",
    "\n",
    "def createShingles(df, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "    # create shingles for each route\n",
    "    shingles = []\n",
    "    for index, s in df.iterrows():\n",
    "        idS = s['id']\n",
    "        route = s['route']\n",
    "        shingle = [index]\n",
    "        citiesInRoute = [] # napoli roma milano teramo bergamo [10,4,5,48,12] [10,4,5] [4,5,48] [5,48,12]\n",
    "        merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "        for trip in route:\n",
    "            citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "            #merchandiseInRoute += np.array(list(trip['merchandise'].values()))\n",
    "            for item, n in trip['merchandise'].items():\n",
    "                merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "        if len(route) > 0:\n",
    "            citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "        if len(route) > 0:\n",
    "            merchandiseInRoute = merchandiseInRoute / (maxItemQuantity*len(route))\n",
    "        \n",
    "        hashedShingles = []\n",
    "        for i in range(len(citiesInRoute)-k+1):\n",
    "            # Q: is it correct to set the modulo for the hash function to the number of permutations?\n",
    "            # A: yes, because we want to have a unique hash for each shingle\n",
    "            # Q: would it be better to use a different hash function?\n",
    "            # A: yes, because the modulo function is not a good hash function\n",
    "            hashedShingles.append(hashShingles(citiesInRoute[i:i+k], permutations) )\n",
    "        \n",
    "        shingle.append(np.array(hashedShingles))\n",
    "        \n",
    "        shingle.append(merchandiseInRoute) # quantity hot encoding\n",
    "        \n",
    "        shingles.append(shingle)\n",
    "        \n",
    "    return shingles # [ index, [shingles], [merchandise] ]\n",
    "\n",
    "def create_shingles(s, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    shingle = [s.name]\n",
    "    citiesInRoute = [] \n",
    "    merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "    for trip in route:\n",
    "        citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "        for item, n in trip['merchandise'].items():\n",
    "            merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "    if len(route) > 0:\n",
    "        citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "    if len(route) > 0:\n",
    "        merchandiseInRoute = merchandiseInRoute / (maxItemQuantity*len(route))\n",
    "    \n",
    "    hashedShingles = []\n",
    "    for i in range(len(citiesInRoute)-k+1):\n",
    "        hashedShingles.append(hashShingles(citiesInRoute[i:i+k], permutations))\n",
    "    \n",
    "    shingle.append(np.array(hashedShingles))\n",
    "    shingle.append(merchandiseInRoute)\n",
    "    \n",
    "    return shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles_selfcontained(s, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "        import numpy as np\n",
    "        def hash_shingles(shingles):\n",
    "            # hash shingles\n",
    "            string = \"\"\n",
    "            for shingle in shingles:\n",
    "                string += str(shingle) + \",\"\n",
    "            return hash(string)\n",
    "\n",
    "        idS = s['id']\n",
    "        route = s['route']\n",
    "        shingle = [s.name]\n",
    "        citiesInRoute = []\n",
    "        merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "\n",
    "        for trip in route:\n",
    "            citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "            for item, n in trip['merchandise'].items():\n",
    "                merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "\n",
    "        if len(route) > 0:\n",
    "            citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "\n",
    "        if len(route) > 0:\n",
    "            merchandiseInRoute = merchandiseInRoute / (maxItemQuantity * len(route))\n",
    "\n",
    "        hashedShingles = []\n",
    "\n",
    "        for i in range(len(citiesInRoute) - k + 1):\n",
    "            hashedShingles.append(hash_shingles(citiesInRoute[i:i + k]))\n",
    "\n",
    "        shingle.append(np.array(hashedShingles))\n",
    "        shingle.append(merchandiseInRoute)\n",
    "        return shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "standardSets 10 shape first element (2,) [0, array([ 4166440707458172175, -6039089714524745159], dtype=int64), array([0.3 , 0.15, 0.2 , 0.2 , 0.4 , 0.4 , 0.4 , 0.45, 0.1 , 0.2 ])]\n",
      "\n",
      "actualSets 100 shape first element (2,) [0, array([ 4148090454667575919,  2610782796406785811,  1096096843271519554,\n",
      "        1562436568097173896, -6716958441705324569, -5086330563802034939,\n",
      "       -9111081330876209307], dtype=int64), array([0.48571429, 0.35714286, 0.18571429, 0.35714286, 0.5       ,\n",
      "       0.17142857, 0.21428571, 0.38571429, 0.48571429, 0.35714286])]\n",
      "\n",
      "standardSets: 10\n",
      "actualSets: 100\n"
     ]
    }
   ],
   "source": [
    "standardSets = createShingles(dfStandard, k=K_SHINGLES, uniqueCities=uniqueCities, uniqueItems=uniqueItems, longestRoute=longestRoute, maxItemQuantity=maxItemQuantity, permutations=permutations)\n",
    "actualSets = createShingles(dfActual, k=K_SHINGLES, uniqueCities=uniqueCities, uniqueItems=uniqueItems, longestRoute=longestRoute, maxItemQuantity=maxItemQuantity, permutations=permutations)\n",
    "\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# standardSets = dfStandard.parallel_apply(lambda s: create_shingles_selfcontained(s, K_SHINGLES, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations), axis=1)\n",
    "# standardSets = standardSets.tolist()\n",
    "# actualSets = dfActual.parallel_apply(lambda s: create_shingles_selfcontained(s, K_SHINGLES, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations), axis=1)\n",
    "# actualSets = actualSets.tolist()\n",
    "\n",
    "print(\"\\nstandardSets\", len(standardSets), \"shape first element\", standardSets[0][1].shape, standardSets[0])\n",
    "print(\"\\nactualSets\", len(actualSets),  \"shape first element\", standardSets[0][1].shape, actualSets[0])\n",
    "\n",
    "print(\"\\nstandardSets:\", len(standardSets))\n",
    "print(\"actualSets:\", len(actualSets))\n",
    "\n",
    "assert len(standardSets[0]) == 3, \"The length of the standard set is not equal to 3 (index, shingles, merchandise)\"\n",
    "assert len(standardSets[0][2]) == len(uniqueItems), \"The length of the merchandise vector is not equal to the number of unique items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix(matrix):\n",
    "    intersection = np.dot(matrix, matrix.T)\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    union = row_sums[:, None] + row_sums - intersection\n",
    "    union = np.where(union == 0, 1, union)  # avoid division by zero\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n",
    "\n",
    "def jaccard_similarity_two_matrices(matrix1, matrix2):\n",
    "    #intersection = np.dot(matrix, matrix.T)\n",
    "    intersection = np.dot(matrix1, matrix2.T)\n",
    "    row_sums1 = matrix1.sum(axis=1)\n",
    "    row_sums2 = matrix2.sum(axis=1)\n",
    "    union = row_sums1[:, None] + row_sums2 - intersection\n",
    "    union = np.where(union == 0, 1, union)  # avoid division by zero\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def jaccard_similarity_sparse(matrix1, matrix2):\n",
    "    # Convert dense matrices to sparse matrices (CSR format)\n",
    "    sparse_matrix1 = csr_matrix(matrix1)\n",
    "    sparse_matrix2 = csr_matrix(matrix2)\n",
    "\n",
    "    # Matrix multiplication in CSR format\n",
    "    intersection = sparse_matrix1.dot(sparse_matrix2.T).toarray()\n",
    "\n",
    "    # Row sums using CSR format\n",
    "    row_sums1 = sparse_matrix1.sum(axis=1).A.ravel()\n",
    "    row_sums2 = sparse_matrix2.sum(axis=1).A.ravel()\n",
    "\n",
    "    # Calculate union using the correct formula\n",
    "    union = row_sums1 + row_sums2 - intersection\n",
    "\n",
    "    # Avoid division by zero\n",
    "    union = np.where(union == 0, 1, union)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix_merch(matrix):\n",
    "    print(\"matrix\", matrix.shape)\n",
    "    min_matrix = np.minimum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity\n",
    "\n",
    "def jaccard_similarity_matrices_merch(matrix1, matrix2):\n",
    "    print(\"matrix1\", matrix1.shape)\n",
    "    print(\"matrix2\", matrix2.shape)\n",
    "    \n",
    "    min_matrix = np.minimum(matrix1[:, None, :], matrix2[None, :, :])\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix1[:, None, :], matrix2[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity\n",
    "\n",
    "\n",
    "def create_binary_matrix(routeSets):\n",
    "    # create binary matrix where each row represents a route\n",
    "    uniqueShingles = list(set([shingle for route in routeSets for shingle in route[1]]))\n",
    "    binaryMatrix = np.zeros((len(routeSets), len(uniqueShingles)))\n",
    "    for i, route in enumerate(routeSets):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix[i][uniqueShingles.index(shingle)] = 1\n",
    "    return binaryMatrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function_hash_code(num_of_hashes,n_col,next_prime):\n",
    "  \n",
    "    #coeffA = np.array(pick_random_coefficients(num_of_hashes,max_column_length)).reshape((num_of_hashes,1))\n",
    "    #coeffB = np.array(pick_random_coefficients(num_of_hashes,max_column_length)).reshape((num_of_hashes,1))\n",
    "\n",
    "    coeffA = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "    coeffB = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "\n",
    "    x = np.arange(n_col).reshape((1,n_col))\n",
    "\n",
    "    hash_code = (np.matmul(coeffA,x) + coeffB) % next_prime # (num_of_hashes,n_col) so how each column index is permuted\n",
    "\n",
    "    return hash_code\n",
    "\n",
    "def minhash(u,num_of_hashes):\n",
    "    (n_row, n_col) = u.shape\n",
    "    next_prime = n_col\n",
    "    hash_code = hash_function_hash_code(num_of_hashes,n_col,next_prime)\n",
    "\n",
    "    signature_array = np.empty(shape = (n_row,num_of_hashes))\n",
    "\n",
    "    #t2 = time.time()\n",
    "    for row in tqdm(range(n_row), desc=\"minhashing\"):\n",
    "        #print(\"row\", row)\n",
    "        ones_index = np.where(u[row,:]==1)[0]\n",
    "        #if len(ones_index) == 0:\n",
    "        signature_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "            #continue\n",
    "        corresponding_hashes = hash_code[:,ones_index]\n",
    "        #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "        #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "        row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "        signature_array[row,:] = row_signature\n",
    "\n",
    "    return signature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW FUNCTIONS FOR TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_matrices(routeSet1, routeSet2):\n",
    "    # create binary matrix where each row represents a route\n",
    "    uniqueShinglesBoth = list(set([shingle for route in routeSet1 for shingle in route[1]] + [shingle for route in routeSet2 for shingle in route[1]]))\n",
    "    binaryMatrix1 = np.zeros((len(routeSet1), len(uniqueShinglesBoth)))\n",
    "    binaryMatrix2 = np.zeros((len(routeSet2), len(uniqueShinglesBoth)))\n",
    "    for i, route in enumerate(routeSet1):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix1[i][uniqueShinglesBoth.index(shingle)] = 1\n",
    "            \n",
    "    for i, route in enumerate(routeSet2):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix2[i][uniqueShinglesBoth.index(shingle)] = 1\n",
    "    return binaryMatrix1, binaryMatrix2\n",
    "\n",
    "def find_num_hashes_minhash(matrix):\n",
    "    if matrix.shape[1]<150:\n",
    "        num_hash_functions = matrix.shape[1]\n",
    "    elif matrix.shape[1]<500:\n",
    "        num_hash_functions = matrix.shape[1]//2\n",
    "    elif matrix.shape[1] < 1000:\n",
    "        num_hash_functions = matrix.shape[1]//10\n",
    "    elif matrix.shape[1] < 10_000:\n",
    "        num_hash_functions = 150\n",
    "    elif matrix.shape[1] < 100_000:\n",
    "        num_hash_functions = 250\n",
    "    else:\n",
    "        num_hash_functions = 300\n",
    "    return num_hash_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function_hash_code(num_of_hashes,n_col,next_prime):\n",
    "\n",
    "    coeffA = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "    coeffB = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "\n",
    "    x = np.arange(n_col).reshape((1,n_col))\n",
    "\n",
    "    hash_code = (np.matmul(coeffA,x) + coeffB) % next_prime # (num_of_hashes,n_col) so how each column index is permuted\n",
    "\n",
    "    return hash_code\n",
    "\n",
    "def minhash(u,num_of_hashes):\n",
    "    (n_row, n_col) = u.shape\n",
    "    next_prime = n_col\n",
    "    hash_code = hash_function_hash_code(num_of_hashes,n_col,next_prime)\n",
    "\n",
    "    signature_array = np.empty(shape = (n_row,num_of_hashes))\n",
    "\n",
    "    #t2 = time.time()\n",
    "\n",
    "    for row in tqdm(range(n_row), desc=\"minhashing\"):\n",
    "        #print(\"row\", row)\n",
    "        ones_index = np.where(u[row,:]==1)[0]\n",
    "        #if len(ones_index) == 0:\n",
    "        signature_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "            #continue\n",
    "        corresponding_hashes = hash_code[:,ones_index]\n",
    "        #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "        #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "        row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "        signature_array[row,:] = row_signature\n",
    "\n",
    "    return signature_array\n",
    "\n",
    "def find_band_and_row_values(columns, threshold):\n",
    "    previous_b = 1\n",
    "    previous_r = columns\n",
    "    for b in range(1, columns + 1):\n",
    "        if columns % b == 0:\n",
    "            r = columns // b\n",
    "            if (1 / b) ** (1 / r)  <= threshold:\n",
    "                if np.abs((1 / previous_b) ** (1 / previous_r) - threshold) < np.abs((1 / b) ** (1 / r) - threshold):\n",
    "                    return previous_b, previous_r\n",
    "                return b, r\n",
    "    return columns, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_minhash_lsh_route_merch(matrix, matrixMerch, thresh_user=0.2):\n",
    "    #similarity_matrix = csr_matrix((matrix.shape[0], matrix.shape[0]), dtype=np.float64)\n",
    "    #similarity_matrix = lil_matrix((matrix.shape[0], matrix.shape[0]), dtype=np.float64)\n",
    "    pairs = lsh(matrix, thresh_user=thresh_user)\n",
    "    #uniqueRows = np.unique([i for i, j in pairs] + [j for i, j in pairs])\n",
    "    uniqueRowsSet = set([i for i, j in pairs] + [j for i, j in pairs]) # (1,2) (1,4) (1,5)\n",
    "    neverSeen = set([i for i in range(matrix.shape[0])]) - uniqueRowsSet\n",
    "    print(\"neverSeen\", neverSeen)\n",
    "    #print(\"uniqueRows numpy\", len(uniqueRows))\n",
    "    print(\"num of subset of rows to check similarity:\", len(uniqueRowsSet))\n",
    "    #print(\" num of pairs\", len(uniqueRowsSet)*(len(uniqueRowsSet)-1)/2)\n",
    "    print(\" num of pairs\", len(pairs))\n",
    "    print(\" instead of\", matrix.shape[0]*(matrix.shape[0]-1)/2)\n",
    "    print(\"improved by\", (1 - len(pairs) / (matrix.shape[0]*(matrix.shape[0]-1)/2)) *100, \"%\")\n",
    "    \n",
    "        \n",
    "    print(\"Computing jaccard similarity on subset matrix...\")\n",
    "    #print(\"subset matrix\", subset_matrix.shape)\n",
    "\n",
    "    # with ProgressBar(total=len(pairs)) as progress:\n",
    "    #     distance_pairs = compute_subset_similarity_matrix_only_pairs(matrix, matrixMerch, pairs, progress)\n",
    "    \n",
    "    sortedUniqueRowsSet = sorted(list(uniqueRowsSet))\n",
    "    subset_matrix = matrix[sortedUniqueRowsSet]\n",
    "    subset_matrixMerch = matrixMerch[sortedUniqueRowsSet]\n",
    "    print(\"subset_matrix\", subset_matrix.shape, subset_matrix[0])\n",
    "    print(\"subset_matrixMerch\", subset_matrixMerch.shape, subset_matrixMerch[0])\n",
    "    with ProgressBar(total=len(sortedUniqueRowsSet)) as progress:\n",
    "        subset_sim_matrix = compute_subset_similarity_matrix_and_merch(subset_matrix, subset_matrixMerch, progress)\n",
    "    print(\"subset_sim_matrix\", subset_sim_matrix.shape, subset_sim_matrix[0])\n",
    "    print(\"subset_sim_matrix contains nan\", np.isnan(subset_sim_matrix).any())\n",
    "    print(\"nan indices\", len(np.argwhere(np.isnan(subset_sim_matrix))), np.argwhere(np.isnan(subset_sim_matrix)))\n",
    "    \n",
    "    # if len(neverSeen) > 0:\n",
    "    #     for i, n in enumerate(neverSeen):\n",
    "    #         distance_pairs = np.concatenate([distance_pairs, [1]*(matrix.shape[0]-1-i)])\n",
    "        \n",
    "    #     pairs = np.concatenate([pairs, np.array([[i, j] for i,n  in enumerate(neverSeen) for j in range(i, matrix.shape[0]) if i != j])])\n",
    "    #print(\"pairs\", pairs.shape, pairs[-10:])\n",
    "    # map back to original matrix\n",
    "    print(\"Mapping back to original matrix...\")\n",
    "    \n",
    "    lenMatrixNoNeverSeen = matrix.shape[0] - len(neverSeen)\n",
    "    \n",
    "    # remove never seen rows and map indices\n",
    "    map_indices = {}\n",
    "    sortedNeverSeen = sorted(list(neverSeen))\n",
    "    counter = 0\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if i in sortedNeverSeen:\n",
    "            continue\n",
    "        map_indices[i] = counter\n",
    "        counter += 1\n",
    "        \n",
    "    print(\"map_indices\", map_indices)\n",
    "    map_indices_back = {v: k for k, v in map_indices.items()}\n",
    "    \n",
    "  \n",
    "    subset_sim_matrix = csr_matrix(subset_sim_matrix)\n",
    "    \n",
    "    return subset_sim_matrix, map_indices_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(minhash_matrix, thresh_user=0.2):\n",
    "    # Initialize the signature matrix\n",
    "    columns = minhash_matrix.shape[1]\n",
    "    \n",
    "    # Generate the hash functions\n",
    "   # hash_functions = [lambda x, a=a, b=b: (a * x + b) % minhash_matrix.shape[1] for a, b in zip(random.sample(range(1000), bands), random.sample(range(1000), bands))]\n",
    "    hash_function = lambda x: hash(\",\".join([str(x[i]) for i in range(len(x))]))\n",
    "    \n",
    "    # b = bands\n",
    "    # r = columns//bands\n",
    "    b, r = find_band_and_row_values(columns, thresh_user)\n",
    "    # If columns is not divisible by bands\n",
    "    if columns % b != 0:\n",
    "        # Find the closest number that makes it divisible\n",
    "        while columns % b != 0:\n",
    "            b -= 1\n",
    "        r = columns // b\n",
    "    #bands = b\n",
    "        \n",
    "    print(\"final bands\", b)\n",
    "    signature_matrix = np.full((minhash_matrix.shape[0], b), np.inf)\n",
    "    \n",
    "    # if threshold is 0.8,\n",
    "    threshold = (1 / b) ** (1 / r) \n",
    "    print(\"lsh threshold\", threshold)\n",
    "    \n",
    "    # For each band\n",
    "    print(\"Computing hash values of bands...\")\n",
    "    hash_values = np.apply_along_axis(lambda x: hash_function(x) % minhash_matrix.shape[0], 1, minhash_matrix.reshape(-1, r))\n",
    "    # Reshape the hash values to match the signature matrix\n",
    "    hash_values = hash_values.reshape(minhash_matrix.shape[0], b)\n",
    "    # Update the signature matrix\n",
    "    signature_matrix = hash_values\n",
    "            \n",
    "    # find candidate pairs\n",
    "    print(\"Finding candidate pairs...\")\n",
    "    candidate_pairs = []\n",
    "    for i in tqdm(range(signature_matrix.shape[0])):\n",
    "        # Compute the similarity of the current row with all following rows\n",
    "        similarities = np.sum(signature_matrix[i+1:, :] == signature_matrix[i, :], axis=1) / b\n",
    "        # Find the indices of the rows that have a similarity greater than or equal to the threshold\n",
    "        indices = np.nonzero(similarities >= threshold)[0]\n",
    "        # Add the pairs to the candidate pairs\n",
    "        candidate_pairs.extend((i, i+1+index) for index in indices)\n",
    "    \n",
    "    return np.array(candidate_pairs)\n",
    "\n",
    "def lsh_two_matrices(minhash_matrix1, minhash_matrix2, thresh_user=0.2):\n",
    "    # Initialize the signature matrix\n",
    "    columns = minhash_matrix1.shape[1]\n",
    "    \n",
    "    # Generate the hash functions\n",
    "    # hash_functions = [lambda x, a=a, b=b: (a * x + b) % minhash_matrix.shape[1] for a, b in zip(random.sample(range(1000), bands), random.sample(range(1000), bands))]\n",
    "    hash_function = lambda x: hash(\",\".join([str(x[i]) for i in range(len(x))]))\n",
    "    \n",
    "    # b = bands\n",
    "    # r = columns//bands\n",
    "    b, r = find_band_and_row_values(columns, thresh_user)\n",
    "    # If columns is not divisible by bands\n",
    "    if columns % b != 0:\n",
    "        # Find the closest number that makes it divisible\n",
    "        while columns % b != 0:\n",
    "            b -= 1\n",
    "        r = columns // b\n",
    "        \n",
    "    print(\"final bands\", b)\n",
    "    signature_matrix1 = np.full((minhash_matrix1.shape[0], b), np.inf)\n",
    "    signature_matrix2 = np.full((minhash_matrix2.shape[0], b), np.inf)\n",
    "    \n",
    "\n",
    "    threshold = (1 / b) ** (1 / r) \n",
    "    print(\"lsh threshold\", threshold)\n",
    "    \n",
    "    # For each band\n",
    "    print(\"Computing hash values of bands...\")\n",
    "    hash_values1 = np.apply_along_axis(lambda x: hash_function(x) % minhash_matrix1.shape[0], 1, minhash_matrix1.reshape(-1, r))\n",
    "    hash_values2 = np.apply_along_axis(lambda x: hash_function(x) % minhash_matrix2.shape[0], 1, minhash_matrix2.reshape(-1, r))\n",
    "    # Reshape the hash values to match the signature matrix\n",
    "    hash_values1 = hash_values1.reshape(minhash_matrix1.shape[0], b)\n",
    "    hash_values2 = hash_values2.reshape(minhash_matrix2.shape[0], b)\n",
    "    # Update the signature matrix\n",
    "    signature_matrix1 = hash_values1\n",
    "    signature_matrix2 = hash_values2\n",
    "            \n",
    "    # find candidate pairs\n",
    "    print(\"Finding candidate pairs...\")\n",
    "    # similarities_actual=[]\n",
    "    candidate_pairs = np.empty((minhash_matrix1.shape[0], 2))\n",
    "\n",
    "    data=[]\n",
    "    rows=[]\n",
    "    cols=[]\n",
    "\n",
    "    for i in tqdm(range(signature_matrix1.shape[0])):\n",
    "        # Compute the similarity of the current row with all following rows\n",
    "        similarities = np.sum(signature_matrix2 == signature_matrix1[i, :], axis=1) / b\n",
    "        # print(\"similarities\", similarities.shape, similarities)\n",
    "        # Find the indices of the rows that have a similarity greater than or equal to the threshold\n",
    "        indices = np.nonzero(similarities >= threshold)[0]\n",
    "        # print(\"indices\", indices.shape, indices)\n",
    "        data.extend(similarities[indices])\n",
    "        rows.extend([i]*len(indices))\n",
    "        cols.extend(indices)\n",
    "        # indexMax = np.argmax(similarities)\n",
    "        # simMax = similarities[indexMax]\n",
    "        # # Add the pairs to the candidate pairs\n",
    "        # #candidate_pairs.extend((i, i+1+index) for index in indices)\n",
    "        # candidate_pairs[i] = [indexMax, simMax]\n",
    "\n",
    "        # similarities_actual.append(similarities)\n",
    "        \n",
    "\n",
    "    # # Create data array for COO matrix\n",
    "    # data = np.concatenate([subset_sim_matrix[indices_i, indices_j], subset_sim_matrix[indices_i, indices_j]])\n",
    "    \n",
    "    # # Create row and column index arrays for COO matrix\n",
    "    # rows = np.concatenate([indices_i_mapped, indices_j_mapped])\n",
    "    # cols = np.concatenate([indices_j_mapped, indices_i_mapped])\n",
    "    # print(\"data\", data)\n",
    "    # print(\"rows\", rows)\n",
    "    # print(\"cols\", cols)\n",
    "\n",
    "    similarity_matrix = coo_matrix((data, (rows, cols)), shape=(minhash_matrix1.shape[0], minhash_matrix2.shape[0])).tocsr()\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "import numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_37912/1888202274.py:1: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n"
     ]
    }
   ],
   "source": [
    "@jit(cache=True, nogil=True, parallel=True)\n",
    "def compute_distance_pairs_Merch(sim_matrix, matrix1, matrix1Merch, matrix2, matrix2Merch, progress_proxy):\n",
    "    n = sim_matrix.shape[0]\n",
    "    m = sim_matrix.shape[1]\n",
    "    \n",
    "    # print(\"sim_matrix\", sim_matrix.shape)    \n",
    "    # print(numba.typeof(sim_matrix))\n",
    "    # print(numba.typeof(matrix1))\n",
    "    # print(numba.typeof(matrix1Merch))\n",
    "    # print(numba.typeof(matrix2))\n",
    "    # print(numba.typeof(matrix2Merch))\n",
    "    # print(numba.typeof(progress_proxy))\n",
    "\n",
    "\n",
    "    for i in prange(n):\n",
    "        subset1 = matrix1[i].reshape(1, -1) #replicate_row(subset_matrix, i) \n",
    "        # print(\"subset1\", subset1.shape)\n",
    "        subset2 = matrix2[sim_matrix[i].nonzero()[1]]\n",
    "        # print(\"subset2\", subset2.shape)\n",
    "        min_matrix = np.minimum(subset1, subset2)\n",
    "        sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "        \n",
    "        max_matrix = np.maximum(subset1, subset2)\n",
    "        sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "\n",
    "        route_distance = 1 - (np.divide(sum_min_matrix, sum_max_matrix))\n",
    "        # print(\"route_distance\", route_distance.shape)\n",
    "\n",
    "        subset1Merch = matrix1Merch[i].reshape(1, -1) #replicate_row(subset_matrixMerch, i)\n",
    "        subset2Merch = matrix2Merch[sim_matrix[i].nonzero()[1]]\n",
    "        # print(\"subset1Merch\", subset1Merch.shape)\n",
    "        # print(\"subset2Merch\", subset2Merch.shape)\n",
    "\n",
    "        min_matrixMerch = np.minimum(subset1Merch, subset2Merch)\n",
    "        sum_min_matrixMerch = np.sum(min_matrixMerch, axis=-1)\n",
    "\n",
    "        max_matrixMerch = np.maximum(subset1Merch, subset2Merch)\n",
    "        sum_max_matrixMerch = np.sum(max_matrixMerch, axis=-1)\n",
    "\n",
    "        merch_distance = 1 - (np.divide(sum_min_matrixMerch, sum_max_matrixMerch))\n",
    "        # print(\"merch_distance\", merch_distance.shape)\n",
    "\n",
    "\n",
    "\n",
    "        sim_matrix[i,sim_matrix[i].nonzero()[1]] = (0.8) * route_distance + (0.2) * merch_distance\n",
    "\n",
    "        progress_proxy.update(1)\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def distance_minhash_lsh_two_matrices(matrix1, matrix1Merch, matrix2, matrix2Merch, thresh_user=0.2):\n",
    "    \n",
    "    similarity_matrix = lsh_two_matrices(matrix1,matrix2, thresh_user=thresh_user)\n",
    "    # print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix)\n",
    "\n",
    "    # uniqueRowsSet = set([i for i, j in pairs] + [j for i, j in pairs]) # (1,2) (1,4) (1,5)\n",
    "    # neverSeen = set([i for i in range(matrix1.shape[0])]) - uniqueRowsSet\n",
    "    \n",
    "\n",
    "    # sortedUniqueRowsSet = sorted(list(uniqueRowsSet))\n",
    "    # print(\"sortedUniqueRowsSet\", sortedUniqueRowsSet)\n",
    "\n",
    "    # subset_matrix1 = matrix1[sortedUniqueRowsSet]\n",
    "    # subset_matrix1Merch = matrix1Merch[sortedUniqueRowsSet]\n",
    "    # print(\"subset_matrix1\", subset_matrix1.shape, subset_matrix1[0])\n",
    "    # print(\"subset_matrix1Merch\", subset_matrix1Merch.shape, subset_matrix1Merch[0])\n",
    "\n",
    "    # subset_matrix2 = matrix2[sortedUniqueRowsSet]\n",
    "    # subset_matrix2Merch = matrix2Merch[sortedUniqueRowsSet]\n",
    "    # print(\"subset_matrix2\", subset_matrix2.shape, subset_matrix2[0])\n",
    "    # print(\"subset_matrix2Merch\", subset_matrix2Merch.shape, subset_matrix2Merch[0])\n",
    "\n",
    "    # subset_similarity_matrix = np.full((subset_matrix1.shape[0], subset_matrix2.shape[0]), np.inf)\n",
    "        \n",
    "    print(\"Computing distance  on subset matrix...\")\n",
    "    with ProgressBar(total=matrix1.shape[0]) as progress:\n",
    "        similarity_matrix = compute_distance_pairs_Merch(similarity_matrix, matrix1, matrix1Merch, matrix2, matrix2Merch, progress)\n",
    "        \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating route binary matrix...\n",
      "\n",
      "route_matrix actual (100, 81) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "route_matrix standard (10, 81) [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "ACTUAL\n",
      "Minhashing route matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minhashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 18967.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "route_matrix minhash (100, 82) [ 6. 15.  8.  7. 28.  6. 16.  0.  6.  0. 12.  5.  1.  9. 10. 11.  4.  6.\n",
      "  1.  5.  3. 15.  7.  1. 23.  0.  3.  5.  0.  2.  1. 13.  5. 13.  1.  0.\n",
      " 11. 30.  2. 22.  8.  0. 24.  2.  5. 14.  5.  0.  8.  9.  1.  1.  1.  6.\n",
      " 27.  0.  8.  1.  0.  5. 13.  5. 15. 11.  4. 12.  6. 18. 14. 27. 29.  8.\n",
      " 21.  3. 14.  9.  2.  3.  1.  5.  1. 14.]\n",
      "\n",
      "Creating merchandise binary matrix...\n",
      "\n",
      "merch_matrix (100, 10) [0.48571429 0.35714286 0.18571429 0.35714286 0.5        0.17142857\n",
      " 0.21428571 0.38571429 0.48571429 0.35714286]\n",
      "merch_matrix contains nan False\n",
      "\n",
      "STANDARD\n",
      "Minhashing route matrix standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minhashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2503.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "route_matrix_standard minhash (10, 82) [ 1. 72. 13.  6. 13. 38. 46. 21. 11. 10. 12. 71.  4. 31. 31. 14. 20. 46.\n",
      " 47. 45. 41. 54.  1. 55.  5. 25. 14. 51. 38. 28. 60. 44. 34. 20. 25. 44.\n",
      " 29. 34. 11.  2. 41. 14. 15. 62. 37.  4. 15. 63. 31. 23.  9. 37. 50. 16.\n",
      "  3.  7. 28. 16. 14.  2. 57. 33.  4.  7.  7. 37. 33. 10. 14.  5. 35. 17.\n",
      "  6. 52. 10.  7. 42. 57. 36. 32. 17.  6.]\n",
      "\n",
      "Creating merchandise binary matrix...\n",
      "\n",
      "merch_matrix_standard (100, 10) [0.48571429 0.35714286 0.18571429 0.35714286 0.5        0.17142857\n",
      " 0.21428571 0.38571429 0.48571429 0.35714286]\n",
      "merch_matrix_standard contains nan False\n",
      "Computing distance route matrix actual to standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert routes and merchandise to binary matrices\n",
    "\n",
    "print(\"Creating route binary matrix...\")\n",
    "route_matrix, route_matrix_standard = create_binary_matrices(actualSets, standardSets)\n",
    "print(\"\\nroute_matrix actual\", route_matrix.shape, route_matrix[0])\n",
    "print(\"\\nroute_matrix standard\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "num_hash_functions = find_num_hashes_minhash(route_matrix)\n",
    "\n",
    "print(\"\\nACTUAL\")\n",
    "print(\"Minhashing route matrix...\")    \n",
    "route_matrix = minhash(route_matrix, num_hash_functions if num_hash_functions % 2 == 0 else num_hash_functions + 1)\n",
    "print(\"\\nroute_matrix minhash\", route_matrix.shape, route_matrix[0])\n",
    "\n",
    "print(\"\\nCreating merchandise binary matrix...\")\n",
    "merch_matrix = np.array([s[2] for s in actualSets])\n",
    "print(\"\\nmerch_matrix\", merch_matrix.shape, merch_matrix[0])\n",
    "print(\"merch_matrix contains nan\", np.isnan(merch_matrix).any())\n",
    "\n",
    "print(\"\\nSTANDARD\")\n",
    "print(\"Minhashing route matrix standard...\") \n",
    "route_matrix_standard = minhash(route_matrix_standard, num_hash_functions if num_hash_functions % 2 == 0 else num_hash_functions + 1)\n",
    "print(\"\\nroute_matrix_standard minhash\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "\n",
    "print(\"\\nCreating merchandise binary matrix...\")\n",
    "merch_matrix_standard = np.array([s[2] for s in standardSets])\n",
    "print(\"\\nmerch_matrix_standard\", merch_matrix.shape, merch_matrix[0])\n",
    "print(\"merch_matrix_standard contains nan\", np.isnan(merch_matrix).any())\n",
    "\n",
    "# Essentials for Task 2\n",
    "# standardToActualSetsDistances = None\n",
    "print(\"Computing distance route matrix actual to standard...\")\n",
    "\n",
    "# route_matrix_distance_actual_standard, map_indices_back = distance_minhash_lsh_two_matrices(route_matrix, merch_matrix, route_matrix_standard, merch_matrix_standard, thresh_user=0.0)\n",
    "# print(\"\\nroute_similarity_standard_to_actual\", route_similarity_standard_to_actual.shape, route_similarity_standard_to_actual[0])\n",
    "\n",
    "# merch_similarity_lsh_standard_to_actual = jaccard_similarity_minhash_lsh_two_matrices(merch_matrix, merch_matrix_standard, thresh_user=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final bands 82\n",
      "lsh threshold 0.012195121951219513\n",
      "Computing hash values of bands...\n",
      "Finding candidate pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 19980.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance  on subset matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf217fa94b7243de8da83dd7aefb2dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "route_matrix_distance_actual_standard = distance_minhash_lsh_two_matrices(route_matrix, merch_matrix, route_matrix_standard, merch_matrix_standard, thresh_user=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 792 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value_index = np.argmax(SetsSimilarities, axis=1)\n",
    "max_value = np.max(SetsSimilarities, axis=1)\n",
    "print(\"max_value_index\", max_value_index.shape, max_value_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_similarity_standard_to_actual = jaccard_similarity_minhash_lsh_two_matrices(route_matrix, route_matrix_standard, thresh_user=0.0)\n",
    "# print(\"\\nroute_similarity_standard_to_actual\", route_similarity_standard_to_actual.shape, route_similarity_standard_to_actual[0])\n",
    "\n",
    "\n",
    "# compute Jaccard similarity for each matrix\n",
    "# print(\"Computing Jaccard similarity route matrix...\")\n",
    "# route_similarity = jaccard_similarity_minhash_lsh(route_matrix, thresh_user=0.4)\n",
    "# #route_similarity = jaccard_similarity_matrix(route_matrix)\n",
    "# print(\"\\nroute_similarity\", type(route_similarity), route_similarity.shape,route_similarity[0, 0], route_similarity[0])\n",
    "# #merch_similarity = jaccard_similarity_matrix_merch(merch_matrix)\n",
    "# print(\"Computing Jaccard similarity merchandise matrix...\")\n",
    "# #merch_similarity = similarity_matrix_merch(merch_matrix)\n",
    "# merch_similarity_lsh = jaccard_similarity_minhash_lsh(merch_matrix, thresh_user=0.4)\n",
    "# print(\"\\nmerch_similarity\", type(merch_similarity_lsh), merch_similarity_lsh.shape, merch_similarity_lsh[0])\n",
    "\n",
    "# print(\"Computing Jaccard similarity route matrix...\")\n",
    "# actualSetsDistances, map_indices_back = jaccard_similarity_minhash_lsh_route_merch(route_matrix, merch_matrix, thresh_user=0.4)\n",
    "# #route_similarity = jaccard_similarity_matrix(route_matrix)\n",
    "# print(\"\\nactualSetsDistances\", type(actualSetsDistances), actualSetsDistances.shape,actualSetsDistances[0, 0], actualSetsDistances[0])\n",
    "# print(\"map indices back\", map_indices_back)\n",
    "\n",
    "\n",
    "# # compute final Jaccard distance\n",
    "# print(\"Multiplying Jaccard similarities...\")\n",
    "# actualSetsDistances = (route_similarity.multiply(merch_similarity_lsh))\n",
    "# actualSetsDistances = np.nan_to_num(actualSetsDistances, nan=0)\n",
    "#actualSetsDistances = 1 - actualSetsDistances\n",
    "#print(\"\\nactualSetsDistances\", actualSetsDistances.shape, actualSetsDistances[0, 0], actualSetsDistances[0])\n",
    "\n",
    "# Essentials for Task 2\n",
    "\n",
    "# standardToActualSetsDistances = None\n",
    "# #route_matrix_standard = create_binary_matrix(standardSets)\n",
    "# print(\"\\nroute_matrix_standard\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "# route_matrix_standard = minhash(route_matrix_standard, num_hash_functions if num_hash_functions % 2 == 0 else num_hash_functions + 1)\n",
    "# print(\"\\nroute_matrix_standard minhash\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "\n",
    "# merch_matrix_standard = np.array([s[2] for s in standardSets])\n",
    "\n",
    "# route_similarity_standard_to_actual = jaccard_similarity_minhash_lsh_two_matrices(route_matrix, route_matrix_standard, thresh_user=0.0)\n",
    "# print(\"\\nroute_similarity_standard_to_actual\", route_similarity_standard_to_actual.shape, route_similarity_standard_to_actual[0])\n",
    "\n",
    "# merch_similarity_lsh_standard_to_actual = jaccard_similarity_minhash_lsh_two_matrices(merch_matrix, merch_matrix_standard, thresh_user=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SetsSimilarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37912/3532365516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmax_value_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSetsSimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSetsSimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(\"Index of the biggest value for each row:\", max_value_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(\"Value of the biggest value for each row:\", max_value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SetsSimilarities' is not defined"
     ]
    }
   ],
   "source": [
    "max_value_index = np.argmax(SetsSimilarities, axis=1)\n",
    "max_value = np.max(SetsSimilarities, axis=1)\n",
    "\n",
    "# print(\"Index of the biggest value for each row:\", max_value_index)\n",
    "# print(\"Value of the biggest value for each row:\", max_value)\n",
    "\n",
    "# print(\"len(max_value_index)\", len(max_value_index))\n",
    "# print(\"len(max_value)\", len(max_value))\n",
    "\n",
    "# [index for index, s in dfActual.iterrows() if s['driver'] == driver]\n",
    "# uniqueDrivers\n",
    "driver_indices = {}\n",
    "\n",
    "for i, s in dfActual.iterrows():\n",
    "    driver = s['driver']\n",
    "    \n",
    "    # Check if the driver is already in the dictionary\n",
    "    if driver in driver_indices:\n",
    "        # If yes, append the index to the existing array\n",
    "        driver_indices[driver].append(i)\n",
    "    else:\n",
    "        # If not, create a new array with the current index\n",
    "        driver_indices[driver] = [i]\n",
    "\n",
    "print(\"driver_indices\", driver_indices)\n",
    "print(\"len(driver_indices)\", len(driver_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_value_index = np.argmax(SetsSimilarities, axis=1)\n",
    "# max_value = np.max(SetsSimilarities, axis=1)\n",
    "# \"sroute\": \"s0\",\n",
    "\n",
    "driver_indices = {}\n",
    "driver_standard = {}\n",
    "\n",
    "for i, s in dfActual.iterrows():\n",
    "    driver = s['driver']\n",
    "    route = s['sroute']\n",
    "    \n",
    "    # Check if the driver is already in the dictionary\n",
    "    if driver in driver_indices:\n",
    "        # If yes, append the index to the existing array\n",
    "        driver_indices[driver].append(i)\n",
    "        driver_standard[driver].append(route)\n",
    "    else:\n",
    "        # If not, create a new array with the current index\n",
    "        driver_indices[driver] = [i]\n",
    "        driver_standard[driver] = [route]\n",
    "\n",
    "print(\"driver_indices\", driver_indices)\n",
    "print(\"len(driver_indices)\", len(driver_indices))\n",
    "\n",
    "print(\"driver_standard\", driver_standard)\n",
    "print(\"len(driver_standard)\", len(driver_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(arrya_test)\", len(arrya_test))\n",
    "mean_test = np.mean(arrya_test)\n",
    "print(\"mean_test\", mean_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
