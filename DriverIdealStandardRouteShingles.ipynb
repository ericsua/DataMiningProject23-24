{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2\n",
    "for each driver, creates a list of standard routes in that order so that the higher in the list a standard route is, the least the diversion of the driver will be, and \n",
    "the output of the program is: \n",
    "\n",
    "a file called driver.json that has for each driver, the 5 standard routes routes that if the driver does them, it minimizes the diversion. You can test this by considering as pool of standard routes those that originally the company has and also those that you recommend in the recStandard.json. The file driver.json has the following syntax:\n",
    "[\n",
    "\t{driver:C, routes:[s10, s20, s2, s6, s10}}, \n",
    "\t{driver:A, routes:[s1, s2, s22, s61, s102]}, \n",
    "â€¦.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME:  c:\\Users\\matti\\Desktop\\CODE\\DataMiningProject23-24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print('HOME: ',HOME)\n",
    "\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import lxml\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import HDBSCAN, DBSCAN\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy.sparse import csr_matrix, issparse, lil_matrix, coo_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from numba import njit, prange, jit\n",
    "from numba_progress import ProgressBar\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_FILENAME = \"standard_small_final.json\"\n",
    "ACTUAL_FILENAME = \"actual_small_final.json\"\n",
    "\n",
    "\n",
    "K_SHINGLES = 2\n",
    "ALPHA = 0.7 #TODO: not needed maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading standard data...\n",
      "\n",
      "Reading actual data...\n",
      "\n",
      "Creating standard dataframe...\n",
      "\n",
      "Creating actual dataframe...\n",
      "   id                                              route\n",
      "0  s0  [{'from': 'Livorno', 'to': 'Catania', 'merchan...\n",
      "1  s1  [{'from': 'Catania', 'to': 'Altamura', 'mercha...\n",
      "2  s2  [{'from': 'Barletta', 'to': 'Livorno', 'mercha...\n",
      "3  s3  [{'from': 'Rho', 'to': 'Caltanissetta', 'merch...\n",
      "4  s4  [{'from': 'Altamura', 'to': 'Turin', 'merchand...\n",
      "   id driver sroute                                              route\n",
      "0  a0      I     s0  [{'from': 'Livorno', 'to': 'Catania', 'merchan...\n",
      "1  a1      C     s0  [{'from': 'Livorno', 'to': 'Rimini', 'merchand...\n",
      "2  a2      A     s0  [{'from': 'Livorno', 'to': 'Catania', 'merchan...\n",
      "3  a3      F     s0  [{'from': 'Livorno', 'to': 'Catania', 'merchan...\n",
      "4  a4      I     s0  [{'from': 'Livorno', 'to': 'Catania', 'merchan...\n",
      "\n",
      "Finished preparing standard data\n",
      "\n",
      "Finished preparing actual data\n",
      "\n",
      "Unique cities:  ['Altamura', 'Ancona', 'Anzio', 'Asti', 'Avellino', 'Barletta', 'Bitonto', 'Brescia', 'Caltanissetta', 'Catania', 'Cerignola', 'Cesena', 'Cinisello Balsamo', 'Faenza', 'Fiumicino', 'Florence', 'Gela', 'Guidonia Montecelio', 'Latina', 'Livorno', 'Milan', 'Naples', 'Novara', 'Olbia', 'Padua', 'Parma', 'Perugia', 'Pescara', 'Pistoia', 'Portici', 'Prato', 'Ragusa', 'Rho', 'Rimini', 'Savona', 'Torre del Greco', 'Trapani', 'Turin', 'Venice', 'Vicenza']\n",
      "Unique items:  ['Beach Chair', 'Beef', 'Bell Pepper', 'Board Games', 'Broccoli', 'Chicken', 'Fabric Softener', 'Gloves', 'Granola Bars', 'Grapes', 'Hand Sanitizer', 'Humidifier', 'Markers', 'Mayonnaise', 'Microwave', 'Mop', 'Notepads', 'Peanut Butter', 'Peanuts', 'Pet Food', 'Portable Charger', 'Raincoat', 'Shampoo', 'Soap', 'Soda', 'Spatula', 'Spinach', 'Tissues', 'Toilet Paper', 'Vacuum Cleaner']\n",
      "Unique drivers:  ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
      "standardIds:  ['s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9']\n",
      "\n",
      "Number of cities:  40\n",
      "Number of items:  30\n",
      "\n",
      "Longest route:  36\n",
      "Shortest route:  6\n",
      "\n",
      "Max item quantity:  100\n",
      "\n",
      "Number of three-shingles:  59280\n",
      "\n",
      "2-shingles:  1560\n",
      "2-shingles:  780\n",
      "\n",
      "\u001b[92mK-Shingles used: 2 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load standard and actual data\n",
    "print(\"\\nReading standard data...\")\n",
    "with open(os.path.join('data',STANDARD_FILENAME)) as f:\n",
    "    standard = json.load(f)\n",
    "\n",
    "print(\"\\nReading actual data...\")\n",
    "with open(os.path.join('data', ACTUAL_FILENAME)) as f:\n",
    "    actual = json.load(f)\n",
    "\n",
    "# load the data into a dataframe\n",
    "print(\"\\nCreating standard dataframe...\")\n",
    "dfStandard = pd.DataFrame(standard)\n",
    "print(\"\\nCreating actual dataframe...\")\n",
    "dfActual = pd.DataFrame(actual)\n",
    "\n",
    "# print head of the dataframes\n",
    "print(dfStandard.head())\n",
    "print(dfActual.head())\n",
    "\n",
    "# get the unique cities and items of the standard data\n",
    "cities = []\n",
    "items = []\n",
    "drivers = []\n",
    "longestRoute = 0\n",
    "shortestRoute = np.inf\n",
    "maxItemQuantity = 0\n",
    "\n",
    "standardRefIds = []\n",
    "for index, s in dfStandard.iterrows():\n",
    "    #print(s)\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    standardRefIds.append(int(idS[1]))\n",
    "    for trip in route:\n",
    "        cities.append(trip['from']) \n",
    "        items.extend(trip['merchandise'].keys())\n",
    "        maxItemQuantity = max(maxItemQuantity, max(trip['merchandise'].values()))\n",
    "    if len(route) > 0:\n",
    "        cities.append(route[-1]['to'])\n",
    "        \n",
    "    if len(route) > longestRoute:\n",
    "        longestRoute = len(route)\n",
    "        \n",
    "    if len(route) < shortestRoute:\n",
    "        shortestRoute = len(route)\n",
    "print(\"\\nFinished preparing standard data\")\n",
    "\n",
    "actualRefStandardIds = []\n",
    "for index, s in dfActual.iterrows():\n",
    "    #print(s)\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    idStandard = s['sroute']\n",
    "    drivers.append(s['driver'])\n",
    "    actualRefStandardIds.append(int(idStandard[1]))\n",
    "    for trip in route:\n",
    "        cities.append(trip['from'])\n",
    "        items.extend(trip['merchandise'].keys())\n",
    "        maxItemQuantity = max(maxItemQuantity, max(trip['merchandise'].values()))\n",
    "        \n",
    "    if len(route) > 0:\n",
    "        cities.append(route[-1]['to'])\n",
    "        \n",
    "    if len(route) > longestRoute:\n",
    "        longestRoute = len(route)\n",
    "    \n",
    "    if len(route) < shortestRoute:\n",
    "        shortestRoute = len(route)\n",
    "print(\"\\nFinished preparing actual data\")\n",
    "\n",
    "# find the unique cities and items\n",
    "uniqueCities = sorted(list(set(cities)))\n",
    "#uniqueCities.insert(0, 'NULL')          # add NULL city, for padding vectors with different lengths (trips in routes)\n",
    "uniqueItems = sorted(list(set(items)))\n",
    "uniqueDrivers = sorted(list(set(drivers)))\n",
    "\n",
    "if shortestRoute < 2:\n",
    "    K_SHINGLES = 2\n",
    "\n",
    "threeShingles = []\n",
    "\n",
    "for i, c1 in enumerate(uniqueCities):\n",
    "    for j, c2 in enumerate(uniqueCities):\n",
    "        if i == j:\n",
    "            continue\n",
    "        for k, c3 in enumerate(uniqueCities):\n",
    "            if j == k or i == k:\n",
    "                continue\n",
    "            threeShingles.append([c1, c2, c3])\n",
    "            \n",
    "permutations = math.perm(len(uniqueCities), K_SHINGLES)\n",
    "\n",
    "print(\"\\nUnique cities: \", uniqueCities)\n",
    "print(\"Unique items: \", uniqueItems)\n",
    "print(\"Unique drivers: \", uniqueDrivers)\n",
    "\n",
    "standardIds = dfStandard['id'].tolist()\n",
    "print(\"standardIds: \", standardIds)\n",
    "\n",
    "print(\"\\nNumber of cities: \", len(uniqueCities))\n",
    "print(\"Number of items: \", len(uniqueItems))\n",
    "\n",
    "print(\"\\nLongest route: \", longestRoute)\n",
    "print(\"Shortest route: \", shortestRoute)\n",
    "\n",
    "print(\"\\nMax item quantity: \", maxItemQuantity)\n",
    "\n",
    "print(\"\\nNumber of three-shingles: \", len(threeShingles))\n",
    "\n",
    "print(f\"\\n{K_SHINGLES}-shingles: \", math.perm(len(uniqueCities), K_SHINGLES))\n",
    "print(f\"{K_SHINGLES}-shingles: \", math.comb(len(uniqueCities), K_SHINGLES))\n",
    "\n",
    "print(f\"\\n\\033[92mK-Shingles used: {K_SHINGLES} \\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashShingles(shingles, n):\n",
    "    # hash shingles\n",
    "    string = \"\" \n",
    "    for shingle in shingles:\n",
    "        string += str(shingle) + \",\" # [45, 4, 8] -> \"45,4,8,\"\n",
    "    \n",
    "    return hash(string) #% n\n",
    "\n",
    "def createShingles(df, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "    # create shingles for each route\n",
    "    shingles = []\n",
    "    for index, s in df.iterrows():\n",
    "        idS = s['id']\n",
    "        route = s['route']\n",
    "        shingle = [index]\n",
    "        citiesInRoute = [] # napoli roma milano teramo bergamo [10,4,5,48,12] [10,4,5] [4,5,48] [5,48,12]\n",
    "        merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "        for trip in route:\n",
    "            citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "            #merchandiseInRoute += np.array(list(trip['merchandise'].values()))\n",
    "            for item, n in trip['merchandise'].items():\n",
    "                merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "        if len(route) > 0:\n",
    "            citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "        if len(route) > 0:\n",
    "            merchandiseInRoute = merchandiseInRoute / (maxItemQuantity*len(route))\n",
    "        \n",
    "        hashedShingles = []\n",
    "        for i in range(len(citiesInRoute)-k+1):\n",
    "            # Q: is it correct to set the modulo for the hash function to the number of permutations?\n",
    "            # A: yes, because we want to have a unique hash for each shingle\n",
    "            # Q: would it be better to use a different hash function?\n",
    "            # A: yes, because the modulo function is not a good hash function\n",
    "            hashedShingles.append(hashShingles(citiesInRoute[i:i+k], permutations) )\n",
    "        \n",
    "        shingle.append(np.array(hashedShingles))\n",
    "        \n",
    "        shingle.append(merchandiseInRoute) # quantity hot encoding\n",
    "        \n",
    "        shingles.append(shingle)\n",
    "        \n",
    "    return shingles # [ index, [shingles], [merchandise] ]\n",
    "\n",
    "def create_shingles(s, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    shingle = [s.name]\n",
    "    citiesInRoute = [] \n",
    "    merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "    for trip in route:\n",
    "        citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "        for item, n in trip['merchandise'].items():\n",
    "            merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "    if len(route) > 0:\n",
    "        citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "    if len(route) > 0:\n",
    "        merchandiseInRoute = merchandiseInRoute / (maxItemQuantity*len(route))\n",
    "    \n",
    "    hashedShingles = []\n",
    "    for i in range(len(citiesInRoute)-k+1):\n",
    "        hashedShingles.append(hashShingles(citiesInRoute[i:i+k], permutations))\n",
    "    \n",
    "    shingle.append(np.array(hashedShingles))\n",
    "    shingle.append(merchandiseInRoute)\n",
    "    \n",
    "    return shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles_selfcontained(s, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "        import numpy as np\n",
    "        def hash_shingles(shingles):\n",
    "            # hash shingles\n",
    "            string = \"\"\n",
    "            for shingle in shingles:\n",
    "                string += str(shingle) + \",\"\n",
    "            return hash(string)\n",
    "\n",
    "        idS = s['id']\n",
    "        route = s['route']\n",
    "        shingle = [s.name]\n",
    "        citiesInRoute = []\n",
    "        merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "\n",
    "        for trip in route:\n",
    "            citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "            for item, n in trip['merchandise'].items():\n",
    "                merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "\n",
    "        if len(route) > 0:\n",
    "            citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "\n",
    "        if len(route) > 0:\n",
    "            merchandiseInRoute = merchandiseInRoute / (maxItemQuantity * len(route))\n",
    "\n",
    "        hashedShingles = []\n",
    "\n",
    "        for i in range(len(citiesInRoute) - k + 1):\n",
    "            hashedShingles.append(hash_shingles(citiesInRoute[i:i + k]))\n",
    "\n",
    "        shingle.append(np.array(hashedShingles))\n",
    "        shingle.append(merchandiseInRoute)\n",
    "        return shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "standardSets 10 shape first element (7,) [0, array([  927601937676351067,  7616274575629296849,  5037646654388308800,\n",
      "        -999797332344628461,  1553328499977496622,  5100688397118852643,\n",
      "       -7541361884513732122], dtype=int64), array([0.42428571, 0.31142857, 0.29285714, 0.31857143, 0.31142857,\n",
      "       0.38428571, 0.32857143, 0.29571429, 0.28428571, 0.24714286,\n",
      "       0.26857143, 0.33428571, 0.27      , 0.24428571, 0.34571429,\n",
      "       0.25714286, 0.15      , 0.46428571, 0.43285714, 0.25285714,\n",
      "       0.37285714, 0.36571429, 0.22857143, 0.06142857, 0.20142857,\n",
      "       0.25285714, 0.55428571, 0.40714286, 0.26571429, 0.46714286])]\n",
      "\n",
      "actualSets 100 shape first element (7,) [0, array([  927601937676351067,  4297192775114874019,   389033426465705138,\n",
      "        5037646654388308800,  -999797332344628461,   411325410248094515,\n",
      "       -7541361884513732122], dtype=int64), array([0.29571429, 0.28571429, 0.20285714, 0.15142857, 0.09      ,\n",
      "       0.34285714, 0.41285714, 0.37571429, 0.23      , 0.24714286,\n",
      "       0.22      , 0.27      , 0.16      , 0.21142857, 0.44285714,\n",
      "       0.24      , 0.10285714, 0.33142857, 0.24      , 0.15714286,\n",
      "       0.29142857, 0.22142857, 0.19857143, 0.20142857, 0.13142857,\n",
      "       0.14428571, 0.38285714, 0.25714286, 0.32      , 0.37285714])]\n",
      "\n",
      "standardSets: 10\n",
      "actualSets: 100\n"
     ]
    }
   ],
   "source": [
    "standardSets = createShingles(dfStandard, k=K_SHINGLES, uniqueCities=uniqueCities, uniqueItems=uniqueItems, longestRoute=longestRoute, maxItemQuantity=maxItemQuantity, permutations=permutations)\n",
    "actualSets = createShingles(dfActual, k=K_SHINGLES, uniqueCities=uniqueCities, uniqueItems=uniqueItems, longestRoute=longestRoute, maxItemQuantity=maxItemQuantity, permutations=permutations)\n",
    "\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# standardSets = dfStandard.parallel_apply(lambda s: create_shingles_selfcontained(s, K_SHINGLES, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations), axis=1)\n",
    "# standardSets = standardSets.tolist()\n",
    "# actualSets = dfActual.parallel_apply(lambda s: create_shingles_selfcontained(s, K_SHINGLES, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations), axis=1)\n",
    "# actualSets = actualSets.tolist()\n",
    "\n",
    "print(\"\\nstandardSets\", len(standardSets), \"shape first element\", standardSets[0][1].shape, standardSets[0])\n",
    "print(\"\\nactualSets\", len(actualSets),  \"shape first element\", standardSets[0][1].shape, actualSets[0])\n",
    "\n",
    "print(\"\\nstandardSets:\", len(standardSets))\n",
    "print(\"actualSets:\", len(actualSets))\n",
    "\n",
    "assert len(standardSets[0]) == 3, \"The length of the standard set is not equal to 3 (index, shingles, merchandise)\"\n",
    "assert len(standardSets[0][2]) == len(uniqueItems), \"The length of the merchandise vector is not equal to the number of unique items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix(matrix):\n",
    "    intersection = np.dot(matrix, matrix.T)\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    union = row_sums[:, None] + row_sums - intersection\n",
    "    union = np.where(union == 0, 1, union)  # avoid division by zero\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n",
    "\n",
    "def jaccard_similarity_two_matrices(matrix1, matrix2):\n",
    "    #intersection = np.dot(matrix, matrix.T)\n",
    "    intersection = np.dot(matrix1, matrix2.T)\n",
    "    row_sums1 = matrix1.sum(axis=1)\n",
    "    row_sums2 = matrix2.sum(axis=1)\n",
    "    union = row_sums1[:, None] + row_sums2 - intersection\n",
    "    union = np.where(union == 0, 1, union)  # avoid division by zero\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def jaccard_similarity_sparse(matrix1, matrix2):\n",
    "    # Convert dense matrices to sparse matrices (CSR format)\n",
    "    sparse_matrix1 = csr_matrix(matrix1)\n",
    "    sparse_matrix2 = csr_matrix(matrix2)\n",
    "\n",
    "    # Matrix multiplication in CSR format\n",
    "    intersection = sparse_matrix1.dot(sparse_matrix2.T).toarray()\n",
    "\n",
    "    # Row sums using CSR format\n",
    "    row_sums1 = sparse_matrix1.sum(axis=1).A.ravel()\n",
    "    row_sums2 = sparse_matrix2.sum(axis=1).A.ravel()\n",
    "\n",
    "    # Calculate union using the correct formula\n",
    "    union = row_sums1 + row_sums2 - intersection\n",
    "\n",
    "    # Avoid division by zero\n",
    "    union = np.where(union == 0, 1, union)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix_merch(matrix):\n",
    "    print(\"matrix\", matrix.shape)\n",
    "    min_matrix = np.minimum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity\n",
    "\n",
    "def jaccard_similarity_matrices_merch(matrix1, matrix2):\n",
    "    print(\"matrix1\", matrix1.shape)\n",
    "    print(\"matrix2\", matrix2.shape)\n",
    "    \n",
    "    min_matrix = np.minimum(matrix1[:, None, :], matrix2[None, :, :])\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix1[:, None, :], matrix2[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity\n",
    "\n",
    "\n",
    "def create_binary_matrix(routeSets):\n",
    "    # create binary matrix where each row represents a route\n",
    "    uniqueShingles = list(set([shingle for route in routeSets for shingle in route[1]]))\n",
    "    binaryMatrix = np.zeros((len(routeSets), len(uniqueShingles)))\n",
    "    for i, route in enumerate(routeSets):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix[i][uniqueShingles.index(shingle)] = 1\n",
    "    return binaryMatrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function_hash_code(num_of_hashes,n_col,next_prime):\n",
    "  \n",
    "    #coeffA = np.array(pick_random_coefficients(num_of_hashes,max_column_length)).reshape((num_of_hashes,1))\n",
    "    #coeffB = np.array(pick_random_coefficients(num_of_hashes,max_column_length)).reshape((num_of_hashes,1))\n",
    "\n",
    "    coeffA = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "    coeffB = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "\n",
    "    x = np.arange(n_col).reshape((1,n_col))\n",
    "\n",
    "    hash_code = (np.matmul(coeffA,x) + coeffB) % next_prime # (num_of_hashes,n_col) so how each column index is permuted\n",
    "\n",
    "    return hash_code\n",
    "\n",
    "def minhash(u,num_of_hashes):\n",
    "    (n_row, n_col) = u.shape\n",
    "    next_prime = n_col\n",
    "    hash_code = hash_function_hash_code(num_of_hashes,n_col,next_prime)\n",
    "\n",
    "    signature_array = np.empty(shape = (n_row,num_of_hashes))\n",
    "\n",
    "    #t2 = time.time()\n",
    "    for row in tqdm(range(n_row), desc=\"minhashing\"):\n",
    "        #print(\"row\", row)\n",
    "        ones_index = np.where(u[row,:]==1)[0]\n",
    "        #if len(ones_index) == 0:\n",
    "        signature_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "            #continue\n",
    "        corresponding_hashes = hash_code[:,ones_index]\n",
    "        #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "        #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "        row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "        signature_array[row,:] = row_signature\n",
    "\n",
    "    return signature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW FUNCTIONS FOR TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_matrices(routeSet1, routeSet2):\n",
    "    # create binary matrix where each row represents a route\n",
    "    uniqueShinglesBoth = list(set([shingle for route in routeSet1 for shingle in route[1]] + [shingle for route in routeSet2 for shingle in route[1]]))\n",
    "    binaryMatrix1 = np.zeros((len(routeSet1), len(uniqueShinglesBoth)))\n",
    "    binaryMatrix2 = np.zeros((len(routeSet2), len(uniqueShinglesBoth)))\n",
    "    for i, route in enumerate(routeSet1):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix1[i][uniqueShinglesBoth.index(shingle)] = 1\n",
    "            \n",
    "    for i, route in enumerate(routeSet2):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix2[i][uniqueShinglesBoth.index(shingle)] = 1\n",
    "    return binaryMatrix1, binaryMatrix2\n",
    "\n",
    "def find_num_hashes_minhash(matrix):\n",
    "    if matrix.shape[1]<150:\n",
    "        num_hash_functions = matrix.shape[1]\n",
    "    elif matrix.shape[1]<500:\n",
    "        num_hash_functions = matrix.shape[1]//2\n",
    "    elif matrix.shape[1] < 1000:\n",
    "        num_hash_functions = matrix.shape[1]//10\n",
    "    elif matrix.shape[1] < 10_000:\n",
    "        num_hash_functions = 150\n",
    "    elif matrix.shape[1] < 100_000:\n",
    "        num_hash_functions = 250\n",
    "    else:\n",
    "        num_hash_functions = 300\n",
    "    return num_hash_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function_hash_code(num_of_hashes,n_col,next_prime):\n",
    "\n",
    "    coeffA = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "    coeffB = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "\n",
    "    x = np.arange(n_col).reshape((1,n_col))\n",
    "\n",
    "    hash_code = (np.matmul(coeffA,x) + coeffB) % next_prime # (num_of_hashes,n_col) so how each column index is permuted\n",
    "\n",
    "    return hash_code\n",
    "\n",
    "# def minhash(u,num_of_hashes):\n",
    "#     (n_row, n_col) = u.shape\n",
    "#     next_prime = n_col\n",
    "#     hash_code = hash_function_hash_code(num_of_hashes,n_col,next_prime)\n",
    "\n",
    "#     signature_array = np.empty(shape = (n_row,num_of_hashes))\n",
    "\n",
    "#     #t2 = time.time()\n",
    "\n",
    "#     for row in tqdm(range(n_row), desc=\"minhashing\"):\n",
    "#         #print(\"row\", row)\n",
    "#         ones_index = np.where(u[row,:]==1)[0]\n",
    "#         #if len(ones_index) == 0:\n",
    "#         signature_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "#             #continue\n",
    "#         corresponding_hashes = hash_code[:,ones_index]\n",
    "#         #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "#         #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "#         row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "#         signature_array[row,:] = row_signature\n",
    "\n",
    "#     return signature_array\n",
    "\n",
    "def minhash_matrices(matrix1,matrix2,num_of_hashes):\n",
    "    (n_row, n_col) = matrix1.shape\n",
    "    next_prime = n_col\n",
    "    hash_code = hash_function_hash_code(num_of_hashes,n_col,next_prime)\n",
    "\n",
    "    signature1_array = np.empty(shape = (n_row,num_of_hashes))\n",
    "    signature2_array = np.empty(shape = (matrix2.shape[0],num_of_hashes))\n",
    "\n",
    "    #t2 = time.time()\n",
    "\n",
    "    for row in tqdm(range(n_row), desc=\"minhashing\"):\n",
    "        #print(\"row\", row)\n",
    "        ones_index = np.where(matrix1[row,:]==1)[0]\n",
    "        #if len(ones_index) == 0:\n",
    "        signature1_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "\n",
    "            #continue\n",
    "        corresponding_hashes = hash_code[:,ones_index]\n",
    "        #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "        #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "        row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "        signature1_array[row,:] = row_signature\n",
    "\n",
    "    for row in tqdm(range(matrix2.shape[0]), desc=\"minhashing\"):\n",
    "        #print(\"row\", row)\n",
    "        ones_index = np.where(matrix2[row,:]==1)[0]\n",
    "        #if len(ones_index) == 0:\n",
    "        signature2_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "\n",
    "            #continue\n",
    "        corresponding_hashes = hash_code[:,ones_index]\n",
    "        #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "        #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "        row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "        signature2_array[row,:] = row_signature\n",
    "\n",
    "    return signature1_array, signature2_array\n",
    "\n",
    "def find_band_and_row_values(columns, threshold):\n",
    "    previous_b = 1\n",
    "    previous_r = columns\n",
    "    for b in range(1, columns + 1):\n",
    "        if columns % b == 0:\n",
    "            r = columns // b\n",
    "            if (1 / b) ** (1 / r)  <= threshold:\n",
    "                if np.abs((1 / previous_b) ** (1 / previous_r) - threshold) < np.abs((1 / b) ** (1 / r) - threshold):\n",
    "                    return previous_b, previous_r\n",
    "                return b, r\n",
    "    return columns, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_minhash_lsh_route_merch(matrix, matrixMerch, thresh_user=0.2):\n",
    "    #similarity_matrix = csr_matrix((matrix.shape[0], matrix.shape[0]), dtype=np.float64)\n",
    "    #similarity_matrix = lil_matrix((matrix.shape[0], matrix.shape[0]), dtype=np.float64)\n",
    "    pairs = lsh(matrix, thresh_user=thresh_user)\n",
    "    #uniqueRows = np.unique([i for i, j in pairs] + [j for i, j in pairs])\n",
    "    uniqueRowsSet = set([i for i, j in pairs] + [j for i, j in pairs]) # (1,2) (1,4) (1,5)\n",
    "    neverSeen = set([i for i in range(matrix.shape[0])]) - uniqueRowsSet\n",
    "    print(\"neverSeen\", neverSeen)\n",
    "    #print(\"uniqueRows numpy\", len(uniqueRows))\n",
    "    print(\"num of subset of rows to check similarity:\", len(uniqueRowsSet))\n",
    "    #print(\" num of pairs\", len(uniqueRowsSet)*(len(uniqueRowsSet)-1)/2)\n",
    "    print(\" num of pairs\", len(pairs))\n",
    "    print(\" instead of\", matrix.shape[0]*(matrix.shape[0]-1)/2)\n",
    "    print(\"improved by\", (1 - len(pairs) / (matrix.shape[0]*(matrix.shape[0]-1)/2)) *100, \"%\")\n",
    "    \n",
    "        \n",
    "    print(\"Computing jaccard similarity on subset matrix...\")\n",
    "    #print(\"subset matrix\", subset_matrix.shape)\n",
    "\n",
    "    # with ProgressBar(total=len(pairs)) as progress:\n",
    "    #     distance_pairs = compute_subset_similarity_matrix_only_pairs(matrix, matrixMerch, pairs, progress)\n",
    "    \n",
    "    sortedUniqueRowsSet = sorted(list(uniqueRowsSet))\n",
    "    subset_matrix = matrix[sortedUniqueRowsSet]\n",
    "    subset_matrixMerch = matrixMerch[sortedUniqueRowsSet]\n",
    "    print(\"subset_matrix\", subset_matrix.shape, subset_matrix[0])\n",
    "    print(\"subset_matrixMerch\", subset_matrixMerch.shape, subset_matrixMerch[0])\n",
    "    with ProgressBar(total=len(sortedUniqueRowsSet)) as progress:\n",
    "        subset_sim_matrix = compute_subset_similarity_matrix_and_merch(subset_matrix, subset_matrixMerch, progress)\n",
    "    print(\"subset_sim_matrix\", subset_sim_matrix.shape, subset_sim_matrix[0])\n",
    "    print(\"subset_sim_matrix contains nan\", np.isnan(subset_sim_matrix).any())\n",
    "    print(\"nan indices\", len(np.argwhere(np.isnan(subset_sim_matrix))), np.argwhere(np.isnan(subset_sim_matrix)))\n",
    "    \n",
    "    # if len(neverSeen) > 0:\n",
    "    #     for i, n in enumerate(neverSeen):\n",
    "    #         distance_pairs = np.concatenate([distance_pairs, [1]*(matrix.shape[0]-1-i)])\n",
    "        \n",
    "    #     pairs = np.concatenate([pairs, np.array([[i, j] for i,n  in enumerate(neverSeen) for j in range(i, matrix.shape[0]) if i != j])])\n",
    "    #print(\"pairs\", pairs.shape, pairs[-10:])\n",
    "    # map back to original matrix\n",
    "    print(\"Mapping back to original matrix...\")\n",
    "    \n",
    "    lenMatrixNoNeverSeen = matrix.shape[0] - len(neverSeen)\n",
    "    \n",
    "    # remove never seen rows and map indices\n",
    "    map_indices = {}\n",
    "    sortedNeverSeen = sorted(list(neverSeen))\n",
    "    counter = 0\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if i in sortedNeverSeen:\n",
    "            continue\n",
    "        map_indices[i] = counter\n",
    "        counter += 1\n",
    "        \n",
    "    print(\"map_indices\", map_indices)\n",
    "    map_indices_back = {v: k for k, v in map_indices.items()}\n",
    "    \n",
    "  \n",
    "    subset_sim_matrix = csr_matrix(subset_sim_matrix)\n",
    "    \n",
    "    return subset_sim_matrix, map_indices_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(minhash_matrix, thresh_user=0.2):\n",
    "    # Initialize the signature matrix\n",
    "    columns = minhash_matrix.shape[1]\n",
    "    \n",
    "    # Generate the hash functions\n",
    "   # hash_functions = [lambda x, a=a, b=b: (a * x + b) % minhash_matrix.shape[1] for a, b in zip(random.sample(range(1000), bands), random.sample(range(1000), bands))]\n",
    "    hash_function = lambda x: hash(\",\".join([str(x[i]) for i in range(len(x))]))\n",
    "    \n",
    "    # b = bands\n",
    "    # r = columns//bands\n",
    "    b, r = find_band_and_row_values(columns, thresh_user)\n",
    "    # If columns is not divisible by bands\n",
    "    if columns % b != 0:\n",
    "        # Find the closest number that makes it divisible\n",
    "        while columns % b != 0:\n",
    "            b -= 1\n",
    "        r = columns // b\n",
    "    #bands = b\n",
    "        \n",
    "    print(\"final bands\", b)\n",
    "    signature_matrix = np.full((minhash_matrix.shape[0], b), np.inf)\n",
    "    \n",
    "    # if threshold is 0.8,\n",
    "    threshold = (1 / b) ** (1 / r) \n",
    "    print(\"lsh threshold\", threshold)\n",
    "    \n",
    "    # For each band\n",
    "    print(\"Computing hash values of bands...\")\n",
    "    hash_values = np.apply_along_axis(lambda x: hash_function(x) % minhash_matrix.shape[0], 1, minhash_matrix.reshape(-1, r))\n",
    "    # Reshape the hash values to match the signature matrix\n",
    "    hash_values = hash_values.reshape(minhash_matrix.shape[0], b)\n",
    "    # Update the signature matrix\n",
    "    signature_matrix = hash_values\n",
    "            \n",
    "    # find candidate pairs\n",
    "    print(\"Finding candidate pairs...\")\n",
    "    candidate_pairs = []\n",
    "    for i in tqdm(range(signature_matrix.shape[0])):\n",
    "        # Compute the similarity of the current row with all following rows\n",
    "        similarities = np.sum(signature_matrix[i+1:, :] == signature_matrix[i, :], axis=1) / b\n",
    "        # Find the indices of the rows that have a similarity greater than or equal to the threshold\n",
    "        indices = np.nonzero(similarities >= threshold)[0]\n",
    "        # Add the pairs to the candidate pairs\n",
    "        candidate_pairs.extend((i, i+1+index) for index in indices)\n",
    "    \n",
    "    return np.array(candidate_pairs)\n",
    "\n",
    "def lsh_two_matrices(minhash_matrix1, minhash_matrix2, thresh_user=0.2):\n",
    "    # Initialize the signature matrix\n",
    "    columns = minhash_matrix1.shape[1]\n",
    "    \n",
    "    # Generate the hash functions\n",
    "    # hash_functions = [lambda x, a=a, b=b: (a * x + b) % minhash_matrix.shape[1] for a, b in zip(random.sample(range(1000), bands), random.sample(range(1000), bands))]\n",
    "    # hash_function = lambda x: hash(\",\".join([str(x[i]) for i in range(len(x))]))\n",
    "    \n",
    "    def hash_function(x):\n",
    "        # print(\"x\",x)\n",
    "        var = hash(\",\".join([str(x[i]) for i in range(len(x))]))\n",
    "        # print (\"str x \", (\",\".join([(x[i]) for i in range(len(x))])))\n",
    "        # print (\"var\", var)\n",
    "        return var % minhash_matrix1.shape[0]\n",
    "\n",
    "\n",
    "    # b = bands\n",
    "    # r = columns//bands\n",
    "    b, r = find_band_and_row_values(columns, thresh_user)\n",
    "    # If columns is not divisible by bands\n",
    "    if columns % b != 0:\n",
    "        # Find the closest number that makes it divisible\n",
    "        while columns % b != 0:\n",
    "            b -= 1\n",
    "        r = columns // b\n",
    "        \n",
    "    print(\"final bands\", b)\n",
    "    signature_matrix1 = np.full((minhash_matrix1.shape[0], b), np.inf)\n",
    "    signature_matrix2 = np.full((minhash_matrix2.shape[0], b), np.inf)\n",
    "    \n",
    "\n",
    "    threshold = (1 / b) ** (1 / r) \n",
    "    print(\"lsh threshold\", threshold)\n",
    "    \n",
    "    # # For each band\n",
    "    # print(\"Computing hash values of bands...\")\n",
    "    # hash_values1 = np.apply_along_axis(lambda x: hash_function(x) % minhash_matrix1.shape[0], 1, minhash_matrix1.reshape(-1, r))\n",
    "    # print(\"hash_values1\", hash_values1.shape, hash_values1)\n",
    "    # hash_values2 = np.apply_along_axis(lambda x: hash_function(x) % minhash_matrix2.shape[0], 1, minhash_matrix2.reshape(-1, r))\n",
    "    # print(\"hash_values2\", hash_values2.shape, hash_values2)\n",
    "\n",
    "    print(\"minhash_matrix1.reshape(-1, r).shape\",minhash_matrix1.reshape(-1, r).shape)\n",
    "\n",
    "    # For each band\n",
    "    print(\"Computing hash values of bands...\")\n",
    "    hash_values1 = np.apply_along_axis(hash_function, 1, minhash_matrix1.reshape(-1, r))\n",
    "    # print(\"hash_values1\", hash_values1.shape, hash_values1)\n",
    "    hash_values2 = np.apply_along_axis(hash_function, 1, minhash_matrix2.reshape(-1, r))\n",
    "    # print(\"hash_values2\", hash_values2.shape, hash_values2)\n",
    "\n",
    "\n",
    "    # Reshape the hash values to match the signature matrix\n",
    "    hash_values1 = hash_values1.reshape(minhash_matrix1.shape[0], b)\n",
    "    # print(\"hash_values1\", hash_values1.shape, hash_values1)\n",
    "    hash_values2 = hash_values2.reshape(minhash_matrix2.shape[0], b)\n",
    "    # print(\"hash_values2\", hash_values2.shape, hash_values2) \n",
    "    # Update the signature matrix\n",
    "    signature_matrix1 = hash_values1\n",
    "    signature_matrix2 = hash_values2\n",
    "    \n",
    "    \n",
    "    # find candidate pairs\n",
    "    print(\"Finding candidate pairs...\")\n",
    "    # similarities_actual=[]\n",
    "    # candidate_pairs = np.empty((minhash_matrix1.shape[0], 2))\n",
    "\n",
    "    data=[]\n",
    "    rows=[]\n",
    "    cols=[]\n",
    "\n",
    "    for i in tqdm(range(signature_matrix1.shape[0])):\n",
    "        # Compute the similarity of the current row with all following rows\n",
    "        similarities = np.sum(signature_matrix2 == signature_matrix1[i, :], axis=1) / b\n",
    "        # print(\"similarities\", similarities.shape, similarities)\n",
    "        # Find the indices of the rows that have a similarity greater than or equal to the threshold\n",
    "        indices = np.nonzero(similarities >= threshold)[0]\n",
    "        # print(\"indices\", indices.shape, indices)\n",
    "\n",
    "        # print(\"similarities[indices] \",similarities[indices])\n",
    "\n",
    "        data.extend(similarities[indices])\n",
    "        # print(\"data\", data)\n",
    "        rows.extend([i]*len(indices))\n",
    "        # print(\"rows\", rows)\n",
    "        cols.extend(indices)\n",
    "        # print(\"cols\", cols)\n",
    "        # indexMax = np.argmax(similarities)\n",
    "        # simMax = similarities[indexMax]\n",
    "        # # Add the pairs to the candidate pairs\n",
    "        # #candidate_pairs.extend((i, i+1+index) for index in indices)\n",
    "        # candidate_pairs[i] = [indexMax, simMax]\n",
    "        # similarities_actual.append(similarities)\n",
    "\n",
    "        \n",
    "\n",
    "    # # Create data array for COO matrix\n",
    "    # data = np.concatenate([subset_sim_matrix[indices_i, indices_j], subset_sim_matrix[indices_i, indices_j]])\n",
    "    \n",
    "    # # Create row and column index arrays for COO matrix\n",
    "    # rows = np.concatenate([indices_i_mapped, indices_j_mapped])\n",
    "    # cols = np.concatenate([indices_j_mapped, indices_i_mapped])\n",
    "    # print(\"data\", data)\n",
    "    # print(\"rows\", rows)\n",
    "    # print(\"cols\", cols)\n",
    "\n",
    "    similarity_matrix = coo_matrix((data, (rows, cols)), shape=(minhash_matrix1.shape[0], minhash_matrix2.shape[0])).tocsr()\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a==b [[ True]\n",
      " [ True]\n",
      " [False]]\n",
      "similarities [0.33333333 0.33333333 0.        ]\n",
      "indices [0 1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1],[2],[3]])\n",
    "b = np.array([[1],[2],[2]])\n",
    "print(\"a==b\", a[:]==b[:])\n",
    "similarities = np.sum(b == a, axis=1)/3\n",
    "print(\"similarities\", similarities)\n",
    "threshold = 0.1\n",
    "indices = np.nonzero(similarities >= threshold)[0]\n",
    "print(\"indices\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n"
     ]
    }
   ],
   "source": [
    "@jit(cache=True, nogil=True, parallel=True)\n",
    "def compute_distance_pairs_Merch(sim_matrix, matrix1, matrix1Merch, matrix2, matrix2Merch, progress_proxy):\n",
    "    n = sim_matrix.shape[0]\n",
    "    m = sim_matrix.shape[1]\n",
    "    \n",
    "    # print(\"sim_matrix\", sim_matrix.shape)    \n",
    "    # print(numba.typeof(sim_matrix))\n",
    "    # print(numba.typeof(matrix1))\n",
    "    # print(numba.typeof(matrix1Merch))\n",
    "    # print(numba.typeof(matrix2))\n",
    "    # print(numba.typeof(matrix2Merch))\n",
    "    # print(numba.typeof(progress_proxy))\n",
    "\n",
    "\n",
    "    for i in prange(n):\n",
    "        subset1 = matrix1[i].reshape(1, -1) #replicate_row(subset_matrix, i) \n",
    "        # print(\"subset1\", subset1.shape)\n",
    "        subset2 = matrix2[sim_matrix[i].nonzero()[1]]\n",
    "        # print(\"subset2\", subset2.shape)\n",
    "        min_matrix = np.minimum(subset1, subset2)\n",
    "        sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "        \n",
    "        max_matrix = np.maximum(subset1, subset2)\n",
    "        sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "\n",
    "        route_distance = (np.divide(sum_min_matrix, sum_max_matrix))\n",
    "        # print(\"route_distance\", route_distance.shape)\n",
    "\n",
    "        subset1Merch = matrix1Merch[i].reshape(1, -1) #replicate_row(subset_matrixMerch, i)\n",
    "        subset2Merch = matrix2Merch[sim_matrix[i].nonzero()[1]]\n",
    "        # print(\"subset1Merch\", subset1Merch.shape)\n",
    "        # print(\"subset2Merch\", subset2Merch.shape)\n",
    "\n",
    "        min_matrixMerch = np.minimum(subset1Merch, subset2Merch)\n",
    "        sum_min_matrixMerch = np.sum(min_matrixMerch, axis=-1)\n",
    "\n",
    "        max_matrixMerch = np.maximum(subset1Merch, subset2Merch)\n",
    "        sum_max_matrixMerch = np.sum(max_matrixMerch, axis=-1)\n",
    "\n",
    "        merch_distance = (np.divide(sum_min_matrixMerch, sum_max_matrixMerch))\n",
    "        # print(\"merch_distance\", merch_distance.shape)\n",
    "\n",
    "\n",
    "\n",
    "        sim_matrix[i,sim_matrix[i].nonzero()[1]] = (0.8) * route_distance + (0.2) * merch_distance\n",
    "\n",
    "        progress_proxy.update(1)\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def similarity_minhash_lsh_two_matrices(matrix1, matrix1Merch, matrix2, matrix2Merch, thresh_user=0.2):\n",
    "    \n",
    "    similarity_matrix = lsh_two_matrices(matrix1,matrix2, thresh_user=thresh_user)\n",
    "    # print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix[0])\n",
    "    # print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix[1])\n",
    "    # print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix[101])\n",
    "    # print(\"similarity_matrix[0]\", similarity_matrix[0,0])\n",
    "    # print(\"similarity_matrix[9,9]\", similarity_matrix[9,9])\n",
    "\n",
    "    # uniqueRowsSet = set([i for i, j in pairs] + [j for i, j in pairs]) # (1,2) (1,4) (1,5)\n",
    "    # neverSeen = set([i for i in range(matrix1.shape[0])]) - uniqueRowsSet\n",
    "    \n",
    "\n",
    "    # sortedUniqueRowsSet = sorted(list(uniqueRowsSet))\n",
    "    # print(\"sortedUniqueRowsSet\", sortedUniqueRowsSet)\n",
    "\n",
    "    # subset_matrix1 = matrix1[sortedUniqueRowsSet]\n",
    "    # subset_matrix1Merch = matrix1Merch[sortedUniqueRowsSet]\n",
    "    # print(\"subset_matrix1\", subset_matrix1.shape, subset_matrix1[0])\n",
    "    # print(\"subset_matrix1Merch\", subset_matrix1Merch.shape, subset_matrix1Merch[0])\n",
    "\n",
    "    # subset_matrix2 = matrix2[sortedUniqueRowsSet]\n",
    "    # subset_matrix2Merch = matrix2Merch[sortedUniqueRowsSet]\n",
    "    # print(\"subset_matrix2\", subset_matrix2.shape, subset_matrix2[0])\n",
    "    # print(\"subset_matrix2Merch\", subset_matrix2Merch.shape, subset_matrix2Merch[0])\n",
    "\n",
    "    # subset_similarity_matrix = np.full((subset_matrix1.shape[0], subset_matrix2.shape[0]), np.inf)\n",
    "        \n",
    "    print(\"Computing distance  on subset matrix...\")\n",
    "    with ProgressBar(total=matrix1.shape[0]) as progress:\n",
    "        similarity_matrix = compute_distance_pairs_Merch(similarity_matrix, matrix1, matrix1Merch, matrix2, matrix2Merch, progress)\n",
    "    \n",
    "    print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix[0])\n",
    "    print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix[1])\n",
    "    # print(\"similarity_matrix\", similarity_matrix.shape, similarity_matrix[101])\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating route binary matrix...\n",
      "\n",
      "route_matrix actual (100, 591) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "route_matrix standard (10, 591) [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Minhashing route matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minhashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 33399.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "minhashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 10043.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "route_matrix minhash (100, 60) [ 44.  62.   1.  27.   0.  47.  26. 175.  22.   9.  86.  25.  47.  31.\n",
      "  70.  39.  31. 276.  31. 124.  24.  25.  15. 179. 103. 127. 150.  92.\n",
      "  74.  49.   2.  60. 180. 204.  96. 136.  39.  42.  12.  73.   0. 129.\n",
      " 185.  52.  83.   3.  84.  46.  25.  15.  40.  12.  32.  15.  21.  16.\n",
      "   3.  74. 179. 147.]\n",
      "\n",
      "route_matrix_standard minhash (10, 60) [  2.  54.   1.  27.   0.  59.  37. 175.  68.   9.  86.  25. 425.  31.\n",
      "  70.  39.  25. 152.  31.   6.  28.  25.  15. 130.  24.  81. 150.  86.\n",
      "  13.  14. 116.  60.  94.  80.  96. 121.  26.  42.  12.  22. 189.   2.\n",
      "  18.  52.  58. 175.  91.  21.  25.  15.  60.  12.  32.  27.  21.  16.\n",
      "  64.  74.  30. 147.]\n",
      "\n",
      "ACTUAL\n",
      "\n",
      "Creating merchandise binary matrix...\n",
      "\n",
      "merch_matrix (100, 30) [0.29571429 0.28571429 0.20285714 0.15142857 0.09       0.34285714\n",
      " 0.41285714 0.37571429 0.23       0.24714286 0.22       0.27\n",
      " 0.16       0.21142857 0.44285714 0.24       0.10285714 0.33142857\n",
      " 0.24       0.15714286 0.29142857 0.22142857 0.19857143 0.20142857\n",
      " 0.13142857 0.14428571 0.38285714 0.25714286 0.32       0.37285714]\n",
      "merch_matrix contains nan False\n",
      "\n",
      "STANDARD\n",
      "\n",
      "Creating merchandise binary matrix...\n",
      "\n",
      "merch_matrix_standard (100, 30) [0.29571429 0.28571429 0.20285714 0.15142857 0.09       0.34285714\n",
      " 0.41285714 0.37571429 0.23       0.24714286 0.22       0.27\n",
      " 0.16       0.21142857 0.44285714 0.24       0.10285714 0.33142857\n",
      " 0.24       0.15714286 0.29142857 0.22142857 0.19857143 0.20142857\n",
      " 0.13142857 0.14428571 0.38285714 0.25714286 0.32       0.37285714]\n",
      "merch_matrix_standard contains nan False\n",
      "Computing distance route matrix actual to standard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert routes and merchandise to binary matrices\n",
    "\n",
    "print(\"Creating route binary matrix...\")\n",
    "route_matrix, route_matrix_standard = create_binary_matrices(actualSets, standardSets)\n",
    "print(\"\\nroute_matrix actual\", route_matrix.shape, route_matrix[0])\n",
    "print(\"\\nroute_matrix standard\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "num_hash_functions = find_num_hashes_minhash(route_matrix)\n",
    "\n",
    "print(\"Minhashing route matrix...\")    \n",
    "route_matrix, route_matrix_standard = minhash_matrices(route_matrix, route_matrix_standard, num_hash_functions if num_hash_functions % 2 == 0 else num_hash_functions + 1)\n",
    "print(\"\\nroute_matrix minhash\", route_matrix.shape, route_matrix[0])\n",
    "print(\"\\nroute_matrix_standard minhash\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "\n",
    "\n",
    "print(\"\\nACTUAL\")\n",
    "\n",
    "print(\"\\nCreating merchandise binary matrix...\")\n",
    "merch_matrix = np.array([s[2] for s in actualSets])\n",
    "print(\"\\nmerch_matrix\", merch_matrix.shape, merch_matrix[0])\n",
    "print(\"merch_matrix contains nan\", np.isnan(merch_matrix).any())\n",
    "\n",
    "print(\"\\nSTANDARD\")\n",
    "\n",
    "print(\"\\nCreating merchandise binary matrix...\")\n",
    "merch_matrix_standard = np.array([s[2] for s in standardSets])\n",
    "print(\"\\nmerch_matrix_standard\", merch_matrix.shape, merch_matrix[0])\n",
    "print(\"merch_matrix_standard contains nan\", np.isnan(merch_matrix).any())\n",
    "\n",
    "# Essentials for Task 2\n",
    "# standardToActualSetsDistances = None\n",
    "print(\"Computing distance route matrix actual to standard...\")\n",
    "\n",
    "# route_matrix_distance_actual_standard, map_indices_back = distance_minhash_lsh_two_matrices(route_matrix, merch_matrix, route_matrix_standard, merch_matrix_standard, thresh_user=0.0)\n",
    "# print(\"\\nroute_similarity_standard_to_actual\", route_similarity_standard_to_actual.shape, route_similarity_standard_to_actual[0])\n",
    "\n",
    "# merch_similarity_lsh_standard_to_actual = jaccard_similarity_minhash_lsh_two_matrices(merch_matrix, merch_matrix_standard, thresh_user=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final bands 30\n",
      "lsh threshold 0.18257418583505536\n",
      "minhash_matrix1.reshape(-1, r).shape (3000, 2)\n",
      "Computing hash values of bands...\n",
      "Finding candidate pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 50249.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing distance  on subset matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a098319f32489d93d32bb3b953a28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"compute_distance_pairs_Merch\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of argument at C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py (1)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 1:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"compute_distance_pairs_Merch\" failed type inference due to: \u001b[1m\u001b[1mCannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 15:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n",
      "c:\\Users\\matti\\anaconda3\\envs\\datamin\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"compute_distance_pairs_Merch\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 1:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\matti\\anaconda3\\envs\\datamin\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 1:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaWarning: \u001b[1mCode running in object mode won't allow parallel execution despite nogil=True.\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaWarning: \u001b[1mCannot cache compiled function \"compute_distance_pairs_Merch\" as it uses lifted code\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"compute_distance_pairs_Merch\" failed type inference due to: \u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of argument at C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py (15)\u001b[0m\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 15:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n",
      "c:\\Users\\matti\\anaconda3\\envs\\datamin\\lib\\site-packages\\numba\\core\\object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"compute_distance_pairs_Merch\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 15:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "c:\\Users\\matti\\anaconda3\\envs\\datamin\\lib\\site-packages\\numba\\core\\object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"..\\..\\..\\AppData\\Local\\Temp\\ipykernel_10356\\4185096283.py\", line 15:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n",
      "C:\\Users\\matti\\AppData\\Local\\Temp/ipykernel_10356/4185096283.py:1: NumbaWarning: \u001b[1mCode running in object mode won't allow parallel execution despite nogil=True.\u001b[0m\n",
      "  @jit(cache=True, nogil=True, parallel=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_matrix (100, 10) \n",
      "similarity_matrix (100, 10) \n"
     ]
    }
   ],
   "source": [
    "route_matrix_similarity_actual_standard = similarity_minhash_lsh_two_matrices(route_matrix, merch_matrix, route_matrix_standard, merch_matrix_standard, thresh_user=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "route_matrix_distance_actual_standard \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nroute_matrix_distance_actual_standard\", route_matrix_similarity_actual_standard[99:101])\n",
    "\n",
    "max_value = np.max(route_matrix_similarity_actual_standard, axis=1).toarray()\n",
    "# print(\"max_value\", max_value.shape, max_value[0:12])\n",
    "\n",
    "max_value_index = np.argmax(route_matrix_similarity_actual_standard, axis=1)\n",
    "# print(\"max_value_index\", max_value_index.shape, max_value_index[0:12])\n",
    "\n",
    "# Set max_value_index to -1 where max_value is zero\n",
    "max_value_index[max_value == 0] = -1\n",
    "\n",
    "# print(\"max_value_index\", max_value_index.shape, max_value_index[0:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(route_matrix_similarity_actual_standard)\n",
    "# print(max_value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, idx in enumerate(max_value_index):\n",
    "#     print(i, \" --> \" ,idx)\n",
    "#     if (i+1)%10==0:\n",
    "#         print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Index of the biggest value for each row:\", max_value_index[850])\n",
    "# print(\"Value of the biggest value for each row:\", max_value[850])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(max_value_index) 100\n",
      "len(max_value) 100\n",
      "driver_indices {'I': [0, 4, 5, 7, 16, 18, 23, 24, 50, 58, 66, 72, 81, 99], 'C': [1, 6, 10, 11, 17, 52, 57, 60, 70, 76, 77, 78, 80, 83, 87, 96, 98], 'A': [2, 13, 14, 25, 37, 38, 71, 93], 'F': [3, 8, 9, 31, 33, 36, 56, 59, 84], 'G': [12, 22, 29, 39, 46, 54, 65, 89], 'E': [15, 49, 55, 73, 79, 86, 88], 'J': [19, 20, 21, 30, 32, 43, 61, 63, 68, 82, 91, 97], 'D': [26, 40, 44, 53, 64, 69, 74, 90, 92], 'H': [27, 28, 34, 35, 42, 45, 67], 'B': [41, 47, 48, 51, 62, 75, 85, 94, 95]}\n",
      "len(driver_indices) 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"len(max_value_index)\", len(max_value_index))\n",
    "print(\"len(max_value)\", len(max_value))\n",
    "\n",
    "# [index for index, s in dfActual.iterrows() if s['driver'] == driver]\n",
    "# uniqueDrivers\n",
    "driver_indices = {}\n",
    "\n",
    "for i, s in dfActual.iterrows():\n",
    "    driver = s['driver']\n",
    "    \n",
    "    # Check if the driver is already in the dictionary\n",
    "    if driver in driver_indices:\n",
    "        # If yes, append the index to the existing array\n",
    "        driver_indices[driver].append(i)\n",
    "    else:\n",
    "        # If not, create a new array with the current index\n",
    "        driver_indices[driver] = [i]\n",
    "\n",
    "print(\"driver_indices\", driver_indices)\n",
    "print(\"len(driver_indices)\", len(driver_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver A\n",
      "driver B\n",
      "driver C\n",
      "driver D\n",
      "driver E\n",
      "driver F\n",
      "driver G\n",
      "driver H\n",
      "driver I\n",
      "driver J\n",
      "JSON driver data has been written to results/driver.json\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of all drivers' routes\n",
    "drivers_routes = {}\n",
    "\n",
    "for driver in uniqueDrivers:\n",
    "    print(\"driver\", driver)\n",
    "\n",
    "    driver_standard_index = np.array(max_value_index[driver_indices[driver]])\n",
    "    driver_max_value = np.array(max_value[driver_indices[driver]])\n",
    "\n",
    "    # Assuming driver_standard_index is a NumPy array\n",
    "    unique_values_index = np.unique(driver_standard_index[~np.isnan(driver_standard_index)]).astype(int)\n",
    "    # print(\"unique_values_index\", unique_values_index)\n",
    "\n",
    "    # Calculate the mean for each unique value\n",
    "    # means = [np.mean(driver_max_value[driver_standard_index == idx]) for idx in unique_values_index if idx != np.nan]\n",
    "    # print(\"means\", means)\n",
    "\n",
    "    weighted_sums = [np.mean(driver_max_value[driver_standard_index == idx]) * np.count_nonzero(driver_standard_index == idx) for idx in unique_values_index if idx!=-1]\n",
    "    # print(\"weighted_sums\", weighted_sums)\n",
    "\n",
    "    best_route_Ids = []\n",
    "    # Print the results for each driver\n",
    "    for idx, mean in zip(unique_values_index, weighted_sums):\n",
    "        # print(f\"Driver: {driver}, Unique Value: {idx}, Mean: {mean}\")\n",
    "        best_route_Ids.append([standardIds[idx], mean])\n",
    "\n",
    "    # Sort the routes by their mean\n",
    "    best_route_Ids.sort(key=lambda x: x[1], reverse=True)\n",
    "    # print(\"best_route_Ids\", best_route_Ids)\n",
    "    \n",
    "    # Keep the top 5 routes\n",
    "    top_5_routes = best_route_Ids[:5]\n",
    "\n",
    "    # Update the driver's routes in the dictionary\n",
    "    drivers_routes[driver] = {'driver': driver, 'routes': [id for id,value in top_5_routes]}\n",
    "\n",
    "# Convert the dictionary to a list for JSON serialization\n",
    "result_list = list(drivers_routes.values())\n",
    "\n",
    "# Write the result to driver.json\n",
    "with open(os.path.join('results', 'driver.json'), 'w') as outfile:\n",
    "    json.dump(result_list, outfile, ensure_ascii=False ,indent=2)\n",
    "\n",
    "print(\"JSON driver data has been written to results/driver.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STANDARD ROUTES x STANDARD ROUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10356/4272238529.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mlol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x+=lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(cache=True, nogil=True, parallel=True)\n",
    "def compute_subset_similarity_matrix_and_merch(matrix, matrixMerch, progress_proxy):\n",
    "    n = matrix.shape[0]\n",
    "    n1 = matrix.shape[1]\n",
    "    m = matrixMerch.shape[1]\n",
    "    similarity_pairs = np.zeros((n,n))\n",
    "    subset2 = matrix\n",
    "    subset2Merch = matrixMerch\n",
    "    squareMatrix = np.full((n, m), 2)\n",
    "    routeWeights = np.full(n, 0.9)\n",
    "    merchWeights = np.full(n, 0.1)\n",
    "    print(\"n\", n, \"m\", m)\n",
    "    print(\"matrix merch\", matrixMerch.shape)\n",
    "    print(\"matrix square\", squareMatrix.shape)\n",
    "    print(\"routeWeights\", routeWeights.shape)\n",
    "    print(\"merchWeights\", merchWeights.shape)\n",
    "    normsSubset2Merch = np.sqrt(np.sum(np.power(subset2Merch, squareMatrix), axis=1))\n",
    "    for i in prange(n):\n",
    "        subset1 = matrix[i].reshape(1, -1) #replicate_row(subset_matrix, i)  \n",
    "        subset1Merch = matrixMerch[i].reshape(1, -1)\n",
    "        #print(\"subset1\", subset1.shape)\n",
    "        #print(\"subset2\", subset2.shape)\n",
    "        \n",
    "        min_matrix = np.minimum(subset1, subset2)\n",
    "        sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "        \n",
    "        max_matrix = np.maximum(subset1, subset2)\n",
    "        sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "        \n",
    "        #print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "        \n",
    "        #print(\"merch1\", subset1Merch.shape)\n",
    "        #print(\"merch2\", subset2Merch.shape)\n",
    "        \n",
    "        #distMerch = 1 - np.abs(np.dot(subset1Merch, subset2Merch.T) / (np.linalg.norm(subset1Merch) * np.linalg.norm(subset2Merch)))\n",
    "        #distMerch = 1 - (((subset1Merch * subset2Merch).sum(axis=1) / (np.sqrt(np.sum(np.power(subset1Merch, squareMatrix),axis=1)) * normsSubset2Merch)) + 1) / 2\n",
    "        \n",
    "        \n",
    "        min_matrix_merch = np.minimum(subset1Merch, subset2Merch)\n",
    "        sum_min_matrix_merch = np.sum(min_matrix_merch, axis=-1)\n",
    "        \n",
    "        max_matrixMerch = np.maximum(subset1Merch, subset2Merch)\n",
    "        sum_max_matrixMerch = np.sum(max_matrixMerch, axis=-1)\n",
    "        distMerch = 1 - (sum_min_matrix_merch / sum_max_matrixMerch)\n",
    "        \n",
    "        # if i == 0 or i == n-1:\n",
    "        #     print(\"i\", i, \"distMerch\", distMerch.shape, distMerch)\n",
    "        #     print(\"i\", i, \"sum_min_matrix\", (1 - (sum_min_matrix / sum_max_matrix)).shape, (1 - (sum_min_matrix / sum_max_matrix)))\n",
    "        #     print(\"i\", i, \"prod\", ((1 - (sum_min_matrix / sum_max_matrix)) * distMerch).shape, ((1 - (sum_min_matrix / sum_max_matrix)) * distMerch))\n",
    "        #print(i, (1 - (sum_min_matrix / sum_max_matrix)) * distMerch)\n",
    "        #similarity_pairs[i] = ((1 - (sum_min_matrix / sum_max_matrix)) + distMerch)/2\n",
    "        routeDistance = 1 - (sum_min_matrix / sum_max_matrix)\n",
    "        similarity_pairs[i] = np.power(routeDistance, routeWeights) * np.power(distMerch, merchWeights)\n",
    "        # if np.isnan(similarity_pairs[i]).any():\n",
    "        #     np.set_printoptions(threshold=10000)\n",
    "        #     # print(\"similarity_pairs[i]\", similarity_pairs[i])\n",
    "        #     print(\"dist merch\", distMerch)\n",
    "        #     # print(\"dist routes\", routeDistance)\n",
    "        #     # print(\"pow1\", np.power(routeDistance, routeWeights))\n",
    "        #     # print(\"pow2\", np.power(distMerch, merchWeights))\n",
    "        #     # print(\"prod\", np.power(routeDistance, routeWeights) * np.power(distMerch, merchWeights))\n",
    "        #     # print(\"powers\", routeWeights, merchWeights)\n",
    "        #     print(\"BROKEN\")\n",
    "        #     return\n",
    "        #similarity_pairs[i] = (1 - (sum_min_matrix / sum_max_matrix)) * (distMerch)\n",
    "        #similarity_pairs[i] = 1 - (sum_min_matrix / sum_max_matrix + distMerch) / 2\n",
    "        # if similarity_pairs[i] >= 1:\n",
    "        #     print(\"similarity_pairs[i]\", similarity_pairs[i])\n",
    "        #     print(\"dist merch\", distMerch, \"cosine \", np.abs(np.dot(subset1Merch, subset2Merch) / (np.linalg.norm(subset1Merch) * np.linalg.norm(subset2Merch))))\n",
    "        #     print(\"dist routes\", (1 - (sum_min_matrix / sum_max_matrix)))\n",
    "        #     print(\"prod\", (1 - (sum_min_matrix / sum_max_matrix)) * distMerch)\n",
    "        progress_proxy.update(1)\n",
    "    return similarity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_minhash_lsh_route_merch(matrix, matrixMerch, thresh_user=0.2):\n",
    "    #similarity_matrix = csr_matrix((matrix.shape[0], matrix.shape[0]), dtype=np.float64)\n",
    "    #similarity_matrix = lil_matrix((matrix.shape[0], matrix.shape[0]), dtype=np.float64)\n",
    "    pairs = lsh(matrix, thresh_user=thresh_user)\n",
    "    #uniqueRows = np.unique([i for i, j in pairs] + [j for i, j in pairs])\n",
    "    uniqueRowsSet = set([i for i, j in pairs] + [j for i, j in pairs]) # (1,2) (1,4) (1,5)\n",
    "    neverSeen = set([i for i in range(matrix.shape[0])]) - uniqueRowsSet\n",
    "    print(\"neverSeen\", neverSeen)\n",
    "    #print(\"uniqueRows numpy\", len(uniqueRows))\n",
    "    print(\"num of subset of rows to check similarity:\", len(uniqueRowsSet))\n",
    "    #print(\" num of pairs\", len(uniqueRowsSet)*(len(uniqueRowsSet)-1)/2)\n",
    "    print(\" num of pairs\", len(pairs))\n",
    "    print(\" instead of\", matrix.shape[0]*(matrix.shape[0]-1)/2)\n",
    "    print(\"improved by\", (1 - len(pairs) / (matrix.shape[0]*(matrix.shape[0]-1)/2)) *100, \"%\")\n",
    "    #print(\"num of pairs\", len(pairs))\n",
    "  \n",
    "        \n",
    "    print(\"Computing jaccard similarity on subset matrix...\")\n",
    "    \n",
    "    sortedUniqueRowsSet = sorted(list(uniqueRowsSet))\n",
    "    subset_matrix = matrix[sortedUniqueRowsSet]\n",
    "    subset_matrixMerch = matrixMerch[sortedUniqueRowsSet]\n",
    "    print(\"subset_matrix\", subset_matrix.shape, subset_matrix[0])\n",
    "    print(\"subset_matrixMerch\", subset_matrixMerch.shape, subset_matrixMerch[0])\n",
    "    with ProgressBar(total=len(sortedUniqueRowsSet)) as progress:\n",
    "        subset_sim_matrix = compute_subset_similarity_matrix_and_merch(subset_matrix, subset_matrixMerch, progress)\n",
    "    print(\"subset_sim_matrix\", subset_sim_matrix.shape, subset_sim_matrix[0])\n",
    "    print(\"subset_sim_matrix contains nan\", np.isnan(subset_sim_matrix).any())\n",
    "    print(\"nan indices\", len(np.argwhere(np.isnan(subset_sim_matrix))), np.argwhere(np.isnan(subset_sim_matrix)))\n",
    "    \n",
    " \n",
    "    print(\"Mapping back to original matrix...\")\n",
    "    \n",
    "    lenMatrixNoNeverSeen = matrix.shape[0] - len(neverSeen)\n",
    "    \n",
    "    # remove never seen rows and map indices\n",
    "    map_indices = {}\n",
    "    sortedNeverSeen = sorted(list(neverSeen))\n",
    "    counter = 0\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if i in sortedNeverSeen:\n",
    "            continue\n",
    "        map_indices[i] = counter\n",
    "        counter += 1\n",
    "        \n",
    "    print(\"map_indices\", map_indices)\n",
    "    map_indices_back = {v: k for k, v in map_indices.items()}\n",
    "    \n",
    "    \n",
    "    #similarity_matrix.setdiag(1)\n",
    "    subset_sim_matrix = csr_matrix(subset_sim_matrix)\n",
    "    \n",
    "    return subset_sim_matrix, map_indices_back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix_merch(matrix):\n",
    "    print(\"matrix\", matrix.shape)\n",
    "    min_matrix = np.minimum(matrix[:, None, :], matrix[None, :, :]) # (10, 100) -> (10, 1, 100) -> (1, 10, 100) -> (10, 10, 100)\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating route binary matrix...\n",
      "\n",
      "route_matrix standard (10, 3098) [0. 0. 0. ... 0. 0. 0.]\n",
      "Minhashing route matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minhashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1254.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "route_matrix minhash (10, 150) [162. 209.   4.  60.  89.  58. 248.   1.  85. 490.  55.  66. 156. 560.\n",
      "  18.  33. 168.   9.  57.  33.  77.  44.  90. 365.   2.  52.  49. 224.\n",
      "  11.  18. 471. 136.  29. 239.  84.  22. 127. 146. 128. 117.   6.   6.\n",
      " 180.  61.  68.  42.   1.  42. 145.   4. 279. 206.  74. 309.  29. 181.\n",
      "  16. 116.  16. 183.   2. 239. 267.  57. 108. 337.  41. 271.  67.  12.\n",
      "  14.  70. 205. 129. 221. 143.  35. 208.  58.  46.  51. 101. 121.  66.\n",
      "  70. 198.  50.  48.  61. 374.  17.  34. 177. 122.  30. 119.  22.  26.\n",
      " 100.  44.   8.  50. 147.  46. 163. 152.  29.  34.  59.  18. 132.  60.\n",
      " 215.  99.  65. 102.  15.  35.  69.  92.  65.  36. 416.  44.  58. 166.\n",
      " 140.  71. 535. 248.  93.  20.  71. 124.  70. 158. 137.  71. 118. 166.\n",
      "   6. 100.  23. 188. 202.  94. 173.  26. 112.  44.]\n",
      "Creating merchandise binary matrix...\n",
      "\n",
      "merch_matrix (10, 50) [[0.24172414 0.34586207 0.24965517 0.28275862 0.33482759 0.21206897\n",
      "  0.19551724 0.20827586 0.26206897 0.22586207 0.23413793 0.22827586\n",
      "  0.21758621 0.27862069 0.24034483 0.21413793 0.2837931  0.25862069\n",
      "  0.20862069 0.32827586 0.25931034 0.21275862 0.20137931 0.26034483\n",
      "  0.31448276 0.25689655 0.15413793 0.21551724 0.3        0.21862069\n",
      "  0.18758621 0.38758621 0.19448276 0.30655172 0.35586207 0.31206897\n",
      "  0.23724138 0.32965517 0.32896552 0.35344828 0.31827586 0.2462069\n",
      "  0.2962069  0.23034483 0.23965517 0.25344828 0.32       0.38862069\n",
      "  0.30689655 0.35655172]\n",
      " [0.33       0.24344828 0.21896552 0.25       0.16482759 0.31482759\n",
      "  0.20206897 0.28655172 0.3162069  0.22103448 0.27344828 0.28724138\n",
      "  0.28172414 0.22413793 0.28862069 0.23724138 0.13344828 0.3062069\n",
      "  0.24       0.17275862 0.21586207 0.23310345 0.34413793 0.25103448\n",
      "  0.24       0.28896552 0.2237931  0.2362069  0.26896552 0.25482759\n",
      "  0.24448276 0.24758621 0.36758621 0.33931034 0.30827586 0.28655172\n",
      "  0.29448276 0.31655172 0.27448276 0.20724138 0.14344828 0.18758621\n",
      "  0.29793103 0.18517241 0.24       0.29724138 0.17068966 0.19448276\n",
      "  0.25896552 0.21517241]\n",
      " [0.1035     0.216      0.2325     0.2255     0.165      0.086\n",
      "  0.2335     0.269      0.1615     0.1585     0.1505     0.108\n",
      "  0.2395     0.1815     0.1625     0.158      0.111      0.127\n",
      "  0.1535     0.1285     0.219      0.145      0.2025     0.146\n",
      "  0.1815     0.251      0.1815     0.2145     0.2425     0.1295\n",
      "  0.1125     0.145      0.287      0.179      0.1555     0.1695\n",
      "  0.122      0.1425     0.315      0.178      0.021      0.2045\n",
      "  0.1525     0.1705     0.101      0.2275     0.24       0.1665\n",
      "  0.254      0.2585    ]\n",
      " [0.07833333 0.25666667 0.16       0.235      0.32333333 0.28666667\n",
      "  0.595      0.25666667 0.25333333 0.17833333 0.34       0.39\n",
      "  0.30666667 0.28666667 0.15333333 0.115      0.19333333 0.15833333\n",
      "  0.28       0.215      0.16666667 0.11833333 0.29833333 0.12833333\n",
      "  0.16333333 0.20833333 0.2        0.07166667 0.245      0.23333333\n",
      "  0.155      0.32       0.18166667 0.16166667 0.40833333 0.065\n",
      "  0.26       0.22666667 0.09333333 0.21333333 0.21       0.38166667\n",
      "  0.24333333 0.23166667 0.205      0.19333333 0.26166667 0.30333333\n",
      "  0.16166667 0.35166667]\n",
      " [0.31545455 0.15090909 0.18363636 0.13       0.37454545 0.22363636\n",
      "  0.16363636 0.42545455 0.46181818 0.23818182 0.37272727 0.28727273\n",
      "  0.05272727 0.30545455 0.27       0.40272727 0.37090909 0.45181818\n",
      "  0.24636364 0.15090909 0.38909091 0.25090909 0.15363636 0.18363636\n",
      "  0.26818182 0.13272727 0.38272727 0.32181818 0.30909091 0.24\n",
      "  0.25090909 0.34181818 0.17545455 0.43454545 0.18636364 0.24363636\n",
      "  0.36090909 0.34818182 0.28181818 0.35727273 0.43545455 0.23454545\n",
      "  0.19454545 0.34818182 0.32727273 0.24545455 0.28       0.34090909\n",
      "  0.19       0.06727273]\n",
      " [0.32       0.36333333 0.42333333 0.15       0.30666667 0.26\n",
      "  0.22       0.44666667 0.52666667 0.36333333 0.59666667 0.39333333\n",
      "  0.39666667 0.05       0.58666667 0.16666667 0.34333333 0.52\n",
      "  0.63333333 0.42333333 0.18333333 0.51333333 0.38333333 0.27666667\n",
      "  0.55666667 0.44       0.15333333 0.38666667 0.21666667 0.29\n",
      "  0.35666667 0.89333333 0.42666667 0.38333333 0.6        0.41666667\n",
      "  0.30666667 0.60333333 0.68       0.56666667 0.26666667 0.46\n",
      "  0.43       0.         0.23333333 0.54       0.09       0.14666667\n",
      "  0.63666667 0.54      ]\n",
      " [0.235625   0.27625    0.1075     0.205      0.24875    0.271875\n",
      "  0.181875   0.1725     0.176875   0.11125    0.221875   0.2825\n",
      "  0.110625   0.034375   0.2325     0.14       0.338125   0.14625\n",
      "  0.225625   0.1725     0.223125   0.0875     0.118125   0.23625\n",
      "  0.07625    0.149375   0.1275     0.156875   0.188125   0.196875\n",
      "  0.113125   0.275      0.233125   0.23       0.280625   0.15875\n",
      "  0.294375   0.189375   0.270625   0.273125   0.30875    0.129375\n",
      "  0.283125   0.148125   0.05875    0.185      0.215625   0.085625\n",
      "  0.225625   0.235     ]\n",
      " [0.20348837 0.25255814 0.16604651 0.21162791 0.21604651 0.22860465\n",
      "  0.17790698 0.26046512 0.21069767 0.10837209 0.26767442 0.25302326\n",
      "  0.18930233 0.26744186 0.2255814  0.20837209 0.21534884 0.24372093\n",
      "  0.25930233 0.19674419 0.30069767 0.17627907 0.09046512 0.32186047\n",
      "  0.17860465 0.13325581 0.16023256 0.16883721 0.24767442 0.26744186\n",
      "  0.2672093  0.26372093 0.36697674 0.16395349 0.37255814 0.2427907\n",
      "  0.25906977 0.15139535 0.21395349 0.15255814 0.22139535 0.21697674\n",
      "  0.23627907 0.17767442 0.20906977 0.21372093 0.27162791 0.26674419\n",
      "  0.19325581 0.21325581]\n",
      " [0.16142857 0.15428571 0.14142857 0.23       0.11       0.23428571\n",
      "  0.40142857 0.10428571 0.41285714 0.         0.12428571 0.14142857\n",
      "  0.20285714 0.07571429 0.22142857 0.48428571 0.41       0.03428571\n",
      "  0.13285714 0.20285714 0.12571429 0.16428571 0.16714286 0.\n",
      "  0.26714286 0.2        0.21714286 0.01142857 0.44428571 0.22142857\n",
      "  0.30285714 0.11285714 0.14428571 0.37571429 0.37285714 0.02285714\n",
      "  0.37142857 0.38857143 0.55857143 0.37285714 0.26285714 0.09142857\n",
      "  0.25714286 0.04428571 0.16428571 0.18       0.22142857 0.18571429\n",
      "  0.38142857 0.09857143]\n",
      " [0.31833333 0.31571429 0.28928571 0.27857143 0.26833333 0.31380952\n",
      "  0.27785714 0.22738095 0.2997619  0.2947619  0.29357143 0.22928571\n",
      "  0.28238095 0.21785714 0.26595238 0.26       0.21285714 0.20738095\n",
      "  0.30095238 0.2702381  0.24666667 0.26047619 0.43833333 0.4052381\n",
      "  0.20904762 0.23142857 0.27619048 0.28166667 0.29333333 0.20952381\n",
      "  0.36190476 0.20761905 0.22690476 0.27785714 0.26952381 0.37619048\n",
      "  0.32190476 0.32       0.28880952 0.2752381  0.20214286 0.19928571\n",
      "  0.29857143 0.24404762 0.20452381 0.28690476 0.28190476 0.2852381\n",
      "  0.235      0.2897619 ]]\n",
      "merch_matrix contains nan False\n",
      "Computing Jaccard similarity route matrix...\n",
      "final bands 50\n",
      "lsh threshold 0.2714417616594907\n",
      "Computing hash values of bands...\n",
      "Finding candidate pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 10027.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neverSeen {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "num of subset of rows to check similarity: 0\n",
      " num of pairs 0\n",
      " instead of 45.0\n",
      "improved by 100.0 %\n",
      "Computing jaccard similarity on subset matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36404/279925224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Computing Jaccard similarity route matrix...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mactualSetsDistances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_indices_back\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjaccard_similarity_minhash_lsh_route_merch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroute_matrix_standard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerch_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresh_user\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#route_similarity = jaccard_similarity_matrix(route_matrix)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nactualSetsDistances\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactualSetsDistances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactualSetsDistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactualSetsDistances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactualSetsDistances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36404/2441142645.py\u001b[0m in \u001b[0;36mjaccard_similarity_minhash_lsh_route_merch\u001b[1;34m(matrix, matrixMerch, thresh_user)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0msubset_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msortedUniqueRowsSet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msubset_matrixMerch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrixMerch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msortedUniqueRowsSet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"subset_matrix\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"subset_matrixMerch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_matrixMerch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_matrixMerch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msortedUniqueRowsSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# convert routes and merchandise to binary matrices\n",
    "# binary matrix where each row represents a route\n",
    "print(\"Creating route binary matrix...\")\n",
    "route_matrix, route_matrix_standard = create_binary_matrices(actualSets, standardSets)\n",
    "print(\"\\nroute_matrix standard\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "\n",
    "print(\"Minhashing route matrix...\")    \n",
    "num_hash_functions = find_num_hashes_minhash(route_matrix_standard)\n",
    "route_matrix_standard = minhash(route_matrix_standard, num_hash_functions if num_hash_functions % 2 == 0 else num_hash_functions + 1)\n",
    "print(\"\\nroute_matrix minhash\", route_matrix_standard.shape, route_matrix_standard[0])\n",
    "# binary matrix where each row represents merchandise\n",
    "\n",
    "print(\"Creating merchandise binary matrix...\")\n",
    "merch_matrix = np.array([s[2] for s in standardSets])\n",
    "\n",
    "print(\"\\nmerch_matrix\", merch_matrix.shape, merch_matrix)\n",
    "print(\"merch_matrix contains nan\", np.isnan(merch_matrix).any())\n",
    "\n",
    "\n",
    "print(\"Computing Jaccard similarity route matrix...\")\n",
    "actualSetsDistances, map_indices_back = jaccard_similarity_minhash_lsh_route_merch(route_matrix_standard, merch_matrix, thresh_user=0.4)\n",
    "#route_similarity = jaccard_similarity_matrix(route_matrix)\n",
    "print(\"\\nactualSetsDistances\", type(actualSetsDistances), actualSetsDistances.shape,actualSetsDistances[0, 0], actualSetsDistances[0])\n",
    "print(\"map indices back\", map_indices_back)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
