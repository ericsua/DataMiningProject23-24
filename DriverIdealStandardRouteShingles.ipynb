{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each driver, creates a list of standard routes in that order so that the higher in the list a standard route is, the least the diversion of the driver will be, and \n",
    "the output of the program is: \n",
    "\n",
    "a file called driver.json that has for each driver, the 5 standard routes routes that if the driver does them, it minimizes the diversion. You can test this by considering as pool of standard routes those that originally the company has and also those that you recommend in the recStandard.json. The file driver.json has the following syntax:\n",
    "[\n",
    "\t{driver:C, routes:[s10, s20, s2, s6, s10}}, \n",
    "\t{driver:A, routes:[s1, s2, s22, s61, s102]}, \n",
    "â€¦.\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME:  c:\\Users\\matti\\Desktop\\CODE\\DataMiningProject23-24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print('HOME: ',HOME)\n",
    "\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import lxml\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.cluster import HDBSCAN, DBSCAN\n",
    "# from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from scipy.sparse import csr_matrix, issparse, lil_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from numba import njit, prange\n",
    "from numba_progress import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_FILE = 'standard_big_new_2.json'\n",
    "ACTUAL_FILE = 'actual_big_new_2.json'\n",
    "\n",
    "\n",
    "K_SHINGLES = 3\n",
    "ALPHA = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading standard data...\n",
      "\n",
      "Reading actual data...\n",
      "\n",
      "Creating standard dataframe...\n",
      "\n",
      "Creating actual dataframe...\n",
      "   id                                              route\n",
      "0  s0  [{'from': 'Caltanissetta', 'to': 'Piacenza', '...\n",
      "1  s1  [{'from': 'Rome', 'to': 'Cerignola', 'merchand...\n",
      "2  s2  [{'from': 'Massa', 'to': 'Treviso', 'merchandi...\n",
      "3  s3  [{'from': 'Cerignola', 'to': 'Perugia', 'merch...\n",
      "4  s4  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "   id driver sroute                                              route\n",
      "0  a0      D     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "1  a1      H     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "2  a2      I     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "3  a3      E     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "4  a4      J     s0  [{'from': 'Massa', 'to': 'Rome', 'merchandise'...\n",
      "\n",
      "Finished preparing standard data\n",
      "\n",
      "Finished preparing actual data\n",
      "\n",
      "Unique cities:  ['Caltanissetta', 'Cerignola', 'Foggia', 'Massa', 'Naples', 'Perugia', 'Piacenza', 'Rome', 'Treviso', 'Vicenza']\n",
      "Unique items:  ['Beach Towel', 'Canned Beans', 'Dumbbells', 'Lawn Mower', 'Lighter Fluid', 'Matches', 'Mayonnaise', 'Pain Relievers', 'Pens', 'Strawberries']\n",
      "Unique drivers:  ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
      "standardIds:  ['s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9']\n",
      "\n",
      "Number of cities:  10\n",
      "Number of items:  10\n",
      "\n",
      "Longest route:  17\n",
      "Shortest route:  1\n",
      "\n",
      "Max item quantity:  10\n",
      "\n",
      "Number of three-shingles:  720\n",
      "\n",
      "2-shingles:  90\n",
      "2-shingles:  45\n",
      "\n",
      "\u001b[92mK-Shingles used: 2 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load standard and actual data\n",
    "print(\"\\nReading standard data...\")\n",
    "with open(os.path.join('data',STANDARD_FILE)) as f:\n",
    "    standard = json.load(f)\n",
    "\n",
    "print(\"\\nReading actual data...\")\n",
    "with open(os.path.join('data', ACTUAL_FILE)) as f:\n",
    "    actual = json.load(f)\n",
    "\n",
    "# load the data into a dataframe\n",
    "print(\"\\nCreating standard dataframe...\")\n",
    "dfStandard = pd.DataFrame(standard)\n",
    "print(\"\\nCreating actual dataframe...\")\n",
    "dfActual = pd.DataFrame(actual)\n",
    "\n",
    "# print head of the dataframes\n",
    "print(dfStandard.head())\n",
    "print(dfActual.head())\n",
    "\n",
    "# get the unique cities and items of the standard data\n",
    "cities = []\n",
    "items = []\n",
    "drivers = []\n",
    "longestRoute = 0\n",
    "shortestRoute = np.inf\n",
    "maxItemQuantity = 0\n",
    "\n",
    "standardRefIds = []\n",
    "for index, s in dfStandard.iterrows():\n",
    "    #print(s)\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    standardRefIds.append(int(idS[1]))\n",
    "    for trip in route:\n",
    "        cities.append(trip['from']) \n",
    "        items.extend(trip['merchandise'].keys())\n",
    "        maxItemQuantity = max(maxItemQuantity, max(trip['merchandise'].values()))\n",
    "    if len(route) > 0:\n",
    "        cities.append(route[-1]['to'])\n",
    "        \n",
    "    if len(route) > longestRoute:\n",
    "        longestRoute = len(route)\n",
    "        \n",
    "    if len(route) < shortestRoute:\n",
    "        shortestRoute = len(route)\n",
    "print(\"\\nFinished preparing standard data\")\n",
    "\n",
    "actualRefStandardIds = []\n",
    "for index, s in dfActual.iterrows():\n",
    "    #print(s)\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    idStandard = s['sroute']\n",
    "    drivers.append(s['driver'])\n",
    "    actualRefStandardIds.append(int(idStandard[1]))\n",
    "    for trip in route:\n",
    "        cities.append(trip['from'])\n",
    "        items.extend(trip['merchandise'].keys())\n",
    "        maxItemQuantity = max(maxItemQuantity, max(trip['merchandise'].values()))\n",
    "        \n",
    "    if len(route) > 0:\n",
    "        cities.append(route[-1]['to'])\n",
    "        \n",
    "    if len(route) > longestRoute:\n",
    "        longestRoute = len(route)\n",
    "    \n",
    "    if len(route) < shortestRoute:\n",
    "        shortestRoute = len(route)\n",
    "print(\"\\nFinished preparing actual data\")\n",
    "\n",
    "# find the unique cities and items\n",
    "uniqueCities = sorted(list(set(cities)))\n",
    "#uniqueCities.insert(0, 'NULL')          # add NULL city, for padding vectors with different lengths (trips in routes)\n",
    "uniqueItems = sorted(list(set(items)))\n",
    "uniqueDrivers = sorted(list(set(drivers)))\n",
    "\n",
    "if shortestRoute < 2:\n",
    "    K_SHINGLES = 2\n",
    "\n",
    "threeShingles = []\n",
    "\n",
    "for i, c1 in enumerate(uniqueCities):\n",
    "    for j, c2 in enumerate(uniqueCities):\n",
    "        if i == j:\n",
    "            continue\n",
    "        for k, c3 in enumerate(uniqueCities):\n",
    "            if j == k or i == k:\n",
    "                continue\n",
    "            threeShingles.append([c1, c2, c3])\n",
    "            \n",
    "permutations = math.perm(len(uniqueCities), K_SHINGLES)\n",
    "\n",
    "print(\"\\nUnique cities: \", uniqueCities)\n",
    "print(\"Unique items: \", uniqueItems)\n",
    "print(\"Unique drivers: \", uniqueDrivers)\n",
    "\n",
    "standardIds = dfStandard['id'].tolist()\n",
    "print(\"standardIds: \", standardIds)\n",
    "\n",
    "print(\"\\nNumber of cities: \", len(uniqueCities))\n",
    "print(\"Number of items: \", len(uniqueItems))\n",
    "\n",
    "print(\"\\nLongest route: \", longestRoute)\n",
    "print(\"Shortest route: \", shortestRoute)\n",
    "\n",
    "print(\"\\nMax item quantity: \", maxItemQuantity)\n",
    "\n",
    "print(\"\\nNumber of three-shingles: \", len(threeShingles))\n",
    "\n",
    "print(f\"\\n{K_SHINGLES}-shingles: \", math.perm(len(uniqueCities), K_SHINGLES))\n",
    "print(f\"{K_SHINGLES}-shingles: \", math.comb(len(uniqueCities), K_SHINGLES))\n",
    "\n",
    "print(f\"\\n\\033[92mK-Shingles used: {K_SHINGLES} \\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashShingles(shingles, n):\n",
    "    # hash shingles\n",
    "    string = \"\" \n",
    "    for shingle in shingles:\n",
    "        string += str(shingle) + \",\" # [45, 4, 8] -> \"45,4,8,\"\n",
    "    \n",
    "    return hash(string) #% n\n",
    "\n",
    "def createShingles(df, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "    # create shingles for each route\n",
    "    shingles = []\n",
    "    for index, s in df.iterrows():\n",
    "        idS = s['id']\n",
    "        route = s['route']\n",
    "        shingle = [index]\n",
    "        citiesInRoute = [] # napoli roma milano teramo bergamo [10,4,5,48,12] [10,4,5] [4,5,48] [5,48,12]\n",
    "        merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "        for trip in route:\n",
    "            citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "            #merchandiseInRoute += np.array(list(trip['merchandise'].values()))\n",
    "            for item, n in trip['merchandise'].items():\n",
    "                merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "        if len(route) > 0:\n",
    "            citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "        if len(route) > 0:\n",
    "            merchandiseInRoute = merchandiseInRoute / (maxItemQuantity*len(route))\n",
    "        \n",
    "        hashedShingles = []\n",
    "        for i in range(len(citiesInRoute)-k+1):\n",
    "            # Q: is it correct to set the modulo for the hash function to the number of permutations?\n",
    "            # A: yes, because we want to have a unique hash for each shingle\n",
    "            # Q: would it be better to use a different hash function?\n",
    "            # A: yes, because the modulo function is not a good hash function\n",
    "            hashedShingles.append(hashShingles(citiesInRoute[i:i+k], permutations) )\n",
    "        \n",
    "        shingle.append(np.array(hashedShingles))\n",
    "        \n",
    "        shingle.append(merchandiseInRoute) # quantity hot encoding\n",
    "        \n",
    "        shingles.append(shingle)\n",
    "        \n",
    "    return shingles # [ index, [shingles], [merchandise] ]\n",
    "\n",
    "def create_shingles(s, k, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations):\n",
    "    idS = s['id']\n",
    "    route = s['route']\n",
    "    shingle = [s.name]\n",
    "    citiesInRoute = [] \n",
    "    merchandiseInRoute = np.zeros(len(uniqueItems))\n",
    "    for trip in route:\n",
    "        citiesInRoute.append(uniqueCities.index(trip['from']))\n",
    "        for item, n in trip['merchandise'].items():\n",
    "            merchandiseInRoute[uniqueItems.index(item)] += n\n",
    "    if len(route) > 0:\n",
    "        citiesInRoute.append(uniqueCities.index(route[-1]['to']))\n",
    "    if len(route) > 0:\n",
    "        merchandiseInRoute = merchandiseInRoute / (maxItemQuantity*len(route))\n",
    "    \n",
    "    hashedShingles = []\n",
    "    for i in range(len(citiesInRoute)-k+1):\n",
    "        hashedShingles.append(hashShingles(citiesInRoute[i:i+k], permutations))\n",
    "    \n",
    "    shingle.append(np.array(hashedShingles))\n",
    "    shingle.append(merchandiseInRoute)\n",
    "    \n",
    "    return shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "standardSets 10 shape first element (2,) [0, array([-6682426417645961081,  6371620475143737465], dtype=int64), array([0.3 , 0.15, 0.2 , 0.2 , 0.4 , 0.4 , 0.4 , 0.45, 0.1 , 0.2 ])]\n",
      "\n",
      "actualSets 100 shape first element (2,) [0, array([ 4210742651727736179,   -60461073256499310,  4449988028164795064,\n",
      "        1103541041183974940, -6870364706471386894, -9215655811683404505,\n",
      "       -3808473816662730513], dtype=int64), array([0.48571429, 0.35714286, 0.18571429, 0.35714286, 0.5       ,\n",
      "       0.17142857, 0.21428571, 0.38571429, 0.48571429, 0.35714286])]\n",
      "\n",
      "standardSets: 10\n",
      "actualSets: 100\n"
     ]
    }
   ],
   "source": [
    "standardSets = createShingles(dfStandard, k=K_SHINGLES, uniqueCities=uniqueCities, uniqueItems=uniqueItems, longestRoute=longestRoute, maxItemQuantity=maxItemQuantity, permutations=permutations)\n",
    "actualSets = createShingles(dfActual, k=K_SHINGLES, uniqueCities=uniqueCities, uniqueItems=uniqueItems, longestRoute=longestRoute, maxItemQuantity=maxItemQuantity, permutations=permutations)\n",
    "#pandarallel.initialize(progress_bar=True)\n",
    "# standardSets = dfStandard.parallel_apply(lambda s: create_shingles(s, K_SHINGLES, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations), axis=1)\n",
    "# standardSets = standardSets.tolist()\n",
    "# actualSets = dfActual.parallel_apply(lambda s: create_shingles(s, K_SHINGLES, uniqueCities, uniqueItems, longestRoute, maxItemQuantity, permutations), axis=1)\n",
    "# actualSets = actualSets.tolist()\n",
    "\n",
    "print(\"\\nstandardSets\", len(standardSets), \"shape first element\", standardSets[0][1].shape, standardSets[0])\n",
    "print(\"\\nactualSets\", len(actualSets),  \"shape first element\", standardSets[0][1].shape, actualSets[0])\n",
    "\n",
    "print(\"\\nstandardSets:\", len(standardSets))\n",
    "print(\"actualSets:\", len(actualSets))\n",
    "\n",
    "assert len(standardSets[0]) == 3, \"The length of the standard set is not equal to 3 (index, shingles, merchandise)\"\n",
    "assert len(standardSets[0][2]) == len(uniqueItems), \"The length of the merchandise vector is not equal to the number of unique items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix(matrix):\n",
    "    intersection = np.dot(matrix, matrix.T)\n",
    "    row_sums = matrix.sum(axis=1)\n",
    "    union = row_sums[:, None] + row_sums - intersection\n",
    "    union = np.where(union == 0, 1, union)  # avoid division by zero\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n",
    "\n",
    "def jaccard_similarity_two_matrices(matrix1, matrix2):\n",
    "    #intersection = np.dot(matrix, matrix.T)\n",
    "    intersection = np.dot(matrix1, matrix2.T)\n",
    "    row_sums1 = matrix1.sum(axis=1)\n",
    "    row_sums2 = matrix2.sum(axis=1)\n",
    "    union = row_sums1[:, None] + row_sums2 - intersection\n",
    "    union = np.where(union == 0, 1, union)  # avoid division by zero\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def jaccard_similarity_sparse(matrix1, matrix2):\n",
    "    # Convert dense matrices to sparse matrices (CSR format)\n",
    "    sparse_matrix1 = csr_matrix(matrix1)\n",
    "    sparse_matrix2 = csr_matrix(matrix2)\n",
    "\n",
    "    # Matrix multiplication in CSR format\n",
    "    intersection = sparse_matrix1.dot(sparse_matrix2.T).toarray()\n",
    "\n",
    "    # Row sums using CSR format\n",
    "    row_sums1 = sparse_matrix1.sum(axis=1).A.ravel()\n",
    "    row_sums2 = sparse_matrix2.sum(axis=1).A.ravel()\n",
    "\n",
    "    # Calculate union using the correct formula\n",
    "    union = row_sums1 + row_sums2 - intersection\n",
    "\n",
    "    # Avoid division by zero\n",
    "    union = np.where(union == 0, 1, union)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix_merch(matrix):\n",
    "    print(\"matrix\", matrix.shape)\n",
    "    min_matrix = np.minimum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix[:, None, :], matrix[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity\n",
    "\n",
    "def jaccard_similarity_matrices_merch(matrix1, matrix2):\n",
    "    print(\"matrix1\", matrix1.shape)\n",
    "    print(\"matrix2\", matrix2.shape)\n",
    "    \n",
    "    min_matrix = np.minimum(matrix1[:, None, :], matrix2[None, :, :])\n",
    "    sum_min_matrix = np.sum(min_matrix, axis=-1)\n",
    "    print(\"sum_min_matrix\", sum_min_matrix.shape)\n",
    "    \n",
    "    max_matrix = np.maximum(matrix1[:, None, :], matrix2[None, :, :])\n",
    "    sum_max_matrix = np.sum(max_matrix, axis=-1)\n",
    "    print(\"sum_max_matrix\", sum_max_matrix.shape)\n",
    "    \n",
    "    jaccard_similarity = sum_min_matrix / sum_max_matrix\n",
    "    return jaccard_similarity\n",
    "\n",
    "\n",
    "def create_binary_matrix(routeSets):\n",
    "    # create binary matrix where each row represents a route\n",
    "    uniqueShingles = list(set([shingle for route in routeSets for shingle in route[1]]))\n",
    "    binaryMatrix = np.zeros((len(routeSets), len(uniqueShingles)))\n",
    "    for i, route in enumerate(routeSets):\n",
    "        for shingle in route[1]:\n",
    "            binaryMatrix[i][uniqueShingles.index(shingle)] = 1\n",
    "    return binaryMatrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function_hash_code(num_of_hashes,n_col,next_prime):\n",
    "  \n",
    "    #coeffA = np.array(pick_random_coefficients(num_of_hashes,max_column_length)).reshape((num_of_hashes,1))\n",
    "    #coeffB = np.array(pick_random_coefficients(num_of_hashes,max_column_length)).reshape((num_of_hashes,1))\n",
    "\n",
    "    coeffA = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "    coeffB = np.array(random.sample(range(0,n_col*100),num_of_hashes)).reshape((num_of_hashes,1))\n",
    "\n",
    "    x = np.arange(n_col).reshape((1,n_col))\n",
    "\n",
    "    hash_code = (np.matmul(coeffA,x) + coeffB) % next_prime # (num_of_hashes,n_col) so how each column index is permuted\n",
    "\n",
    "    return hash_code\n",
    "\n",
    "def minhash(u,num_of_hashes):\n",
    "    (n_row, n_col) = u.shape\n",
    "    next_prime = n_col\n",
    "    hash_code = hash_function_hash_code(num_of_hashes,n_col,next_prime)\n",
    "\n",
    "    signature_array = np.empty(shape = (n_row,num_of_hashes))\n",
    "\n",
    "    #t2 = time.time()\n",
    "    for row in tqdm(range(n_row), desc=\"minhashing\"):\n",
    "        #print(\"row\", row)\n",
    "        ones_index = np.where(u[row,:]==1)[0]\n",
    "        #if len(ones_index) == 0:\n",
    "        signature_array[row,:] = np.zeros((1,num_of_hashes))\n",
    "            #continue\n",
    "        corresponding_hashes = hash_code[:,ones_index]\n",
    "        #print(\"ones_index\", ones_index.shape, ones_index)\n",
    "        #print(\"corresponding_hashes\", corresponding_hashes.shape, corresponding_hashes)\n",
    "        row_signature = np.amin(corresponding_hashes,axis=1).reshape((1,num_of_hashes))\n",
    "\n",
    "        signature_array[row,:] = row_signature\n",
    "\n",
    "    return signature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value_index = np.argmax(SetsSimilarities, axis=1)\n",
    "max_value = np.max(SetsSimilarities, axis=1)\n",
    "\n",
    "# print(\"Index of the biggest value for each row:\", max_value_index)\n",
    "# print(\"Value of the biggest value for each row:\", max_value)\n",
    "\n",
    "# print(\"len(max_value_index)\", len(max_value_index))\n",
    "# print(\"len(max_value)\", len(max_value))\n",
    "\n",
    "# [index for index, s in dfActual.iterrows() if s['driver'] == driver]\n",
    "# uniqueDrivers\n",
    "driver_indices = {}\n",
    "\n",
    "for i, s in dfActual.iterrows():\n",
    "    driver = s['driver']\n",
    "    \n",
    "    # Check if the driver is already in the dictionary\n",
    "    if driver in driver_indices:\n",
    "        # If yes, append the index to the existing array\n",
    "        driver_indices[driver].append(i)\n",
    "    else:\n",
    "        # If not, create a new array with the current index\n",
    "        driver_indices[driver] = [i]\n",
    "\n",
    "print(\"driver_indices\", driver_indices)\n",
    "print(\"len(driver_indices)\", len(driver_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_value_index = np.argmax(SetsSimilarities, axis=1)\n",
    "# max_value = np.max(SetsSimilarities, axis=1)\n",
    "# \"sroute\": \"s0\",\n",
    "\n",
    "driver_indices = {}\n",
    "driver_standard = {}\n",
    "\n",
    "for i, s in dfActual.iterrows():\n",
    "    driver = s['driver']\n",
    "    route = s['sroute']\n",
    "    \n",
    "    # Check if the driver is already in the dictionary\n",
    "    if driver in driver_indices:\n",
    "        # If yes, append the index to the existing array\n",
    "        driver_indices[driver].append(i)\n",
    "        driver_standard[driver].append(route)\n",
    "    else:\n",
    "        # If not, create a new array with the current index\n",
    "        driver_indices[driver] = [i]\n",
    "        driver_standard[driver] = [route]\n",
    "\n",
    "print(\"driver_indices\", driver_indices)\n",
    "print(\"len(driver_indices)\", len(driver_indices))\n",
    "\n",
    "print(\"driver_standard\", driver_standard)\n",
    "print(\"len(driver_standard)\", len(driver_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(arrya_test)\", len(arrya_test))\n",
    "mean_test = np.mean(arrya_test)\n",
    "print(\"mean_test\", mean_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
